{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_analyze_chars():\n",
    "    df = pd.read_csv('malayalam_unicode_chars.csv')\n",
    "    \n",
    "    byte_patterns = []\n",
    "    char_to_bytes = {}\n",
    "    \n",
    "    for char in df['Character']:\n",
    "        # Get UTF-8 bytes for each character\n",
    "        byte_seq = list(char.encode('utf-8'))\n",
    "        byte_patterns.append(byte_seq)\n",
    "        char_to_bytes[char] = byte_seq\n",
    "        \n",
    "    return df, byte_patterns, char_to_bytes\n",
    "\n",
    "\n",
    "def get_byte_pair_frequencies(byte_patterns):\n",
    "    pair_freqs = Counter()\n",
    "    \n",
    "    for pattern in byte_patterns:\n",
    "        for i in range(len(pattern) - 1):\n",
    "            pair = (pattern[i], pattern[i + 1])\n",
    "            pair_freqs[pair] += 1\n",
    "            \n",
    "    return pair_freqs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of characters and their byte patterns:\n",
      "Character: ࡠ, Bytes: [224, 161, 160]\n",
      "Character: ࡡ, Bytes: [224, 161, 161]\n",
      "Character: ࡢ, Bytes: [224, 161, 162]\n",
      "Character: ࡣ, Bytes: [224, 161, 163]\n",
      "Character: ࡤ, Bytes: [224, 161, 164]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df, byte_patterns, char_to_bytes = load_and_analyze_chars()\n",
    "\n",
    "print(\"Sample of characters and their byte patterns:\")\n",
    "for char, bytes_seq in list(char_to_bytes.items())[:5]:\n",
    "    print(f\"Character: {char}, Bytes: {bytes_seq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common byte pairs:\n",
      "Pair: (224, 180), Frequency: 62\n",
      "Pair: (224, 181), Frequency: 56\n",
      "Pair: (224, 161), Frequency: 11\n",
      "Pair: (161, 160), Frequency: 1\n",
      "Pair: (161, 161), Frequency: 1\n",
      "\n",
      "Unique byte lengths:\n",
      "Counter({3: 129})\n",
      "\n",
      "Unique first bytes:\n",
      "[(224, 129)]\n"
     ]
    }
   ],
   "source": [
    "pair_freqs = get_byte_pair_frequencies(byte_patterns)\n",
    "print(\"\\nMost common byte pairs:\")\n",
    "for pair, freq in pair_freqs.most_common(5):\n",
    "    print(f\"Pair: {pair}, Frequency: {freq}\")\n",
    "\n",
    "# Additional analysis for understanding the structure\n",
    "print(\"\\nUnique byte lengths:\")\n",
    "lengths = Counter(len(seq) for seq in byte_patterns)\n",
    "print(lengths)\n",
    "\n",
    "print(\"\\nUnique first bytes:\")\n",
    "first_bytes = Counter(pattern[0] for pattern in byte_patterns)\n",
    "print(sorted(first_bytes.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing new encoding:\n",
      "Character: ࡠ, Bytes: [224, 161, 160], Final token: [259]\n",
      "Character: ࡡ, Bytes: [224, 161, 161], Final token: [260]\n",
      "Character: ࡢ, Bytes: [224, 161, 162], Final token: [261]\n",
      "Character: ࡣ, Bytes: [224, 161, 163], Final token: [262]\n",
      "Character: ࡤ, Bytes: [224, 161, 164], Final token: [263]\n"
     ]
    }
   ],
   "source": [
    "def create_byte_pair_merge_rules():\n",
    "    next_token_id = 256\n",
    "    merge_rules = {}\n",
    "    \n",
    "    # First level merges (first two bytes)\n",
    "    first_level_merges = {}\n",
    "    for char, byte_seq in char_to_bytes.items():\n",
    "        first_pair = (byte_seq[0], byte_seq[1])\n",
    "        if first_pair not in first_level_merges:\n",
    "            first_level_merges[first_pair] = next_token_id\n",
    "            merge_rules[first_pair] = next_token_id\n",
    "            next_token_id += 1\n",
    "    \n",
    "    # Second level merges (merged token + last byte)\n",
    "    for char, byte_seq in char_to_bytes.items():\n",
    "        first_pair = (byte_seq[0], byte_seq[1])\n",
    "        first_token = first_level_merges[first_pair]\n",
    "        second_pair = (first_token, byte_seq[2])\n",
    "        merge_rules[second_pair] = next_token_id\n",
    "        next_token_id += 1\n",
    "    \n",
    "    return merge_rules\n",
    "\n",
    "# Test function\n",
    "def encode_with_merges(byte_seq, merge_rules):\n",
    "    tokens = list(byte_seq)\n",
    "    while len(tokens) > 1:\n",
    "        merged = False\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            if pair in merge_rules:\n",
    "                tokens[i:i+2] = [merge_rules[pair]]\n",
    "                merged = True\n",
    "                break\n",
    "        if not merged:\n",
    "            break\n",
    "    return tokens\n",
    "\n",
    "# Let's test it\n",
    "merge_rules = create_byte_pair_merge_rules()\n",
    "\n",
    "# Test a few characters\n",
    "print(\"Testing new encoding:\")\n",
    "for char in list(char_to_bytes.keys())[:5]:\n",
    "    byte_seq = list(char.encode('utf-8'))\n",
    "    tokens = encode_with_merges(byte_seq, merge_rules)\n",
    "    print(f\"Character: {char}, Bytes: {byte_seq}, Final token: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of merges: 132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((224, 161), 256),\n",
       " ((224, 180), 257),\n",
       " ((224, 181), 258),\n",
       " ((256, 160), 259),\n",
       " ((256, 161), 260),\n",
       " ((256, 162), 261),\n",
       " ((256, 163), 262),\n",
       " ((256, 164), 263),\n",
       " ((256, 165), 264),\n",
       " ((256, 166), 265)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges = list(merge_rules.items())\n",
    "print(f\"Number of merges: {len(merges)}\")  # len(merges)\n",
    "\n",
    "merges[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_bpe import tokenize, decode_tokens, encode_text, calculate_compression_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simple_bpe:Compression Analysis:\n",
      "INFO:simple_bpe:Original size: 18 tokens\n",
      "INFO:simple_bpe:Compressed size: 6 tokens\n",
      "INFO:simple_bpe:Compression ratio: 3.00x\n",
      "INFO:simple_bpe:Space saving: 66.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: മലയാളം\n",
      "Number of characters in input: 6\n",
      "Raw byte tokens: [224, 180, 174, 224, 180, 178, 224, 180, 175, 224, 180, 190, 224, 180, 179, 224, 180, 130]\n",
      "Number of raw tokens: 18\n",
      "After applying merges - tokens: [314, 318, 315, 330, 319, 272]\n",
      "Number of tokens after merges: 6\n",
      "Decoded text: മലയാളം\n",
      "Successful roundtrip: True\n",
      "compression ratio : 3.0\n"
     ]
    }
   ],
   "source": [
    "input_text = \"മലയാളം\"\n",
    "# input_text = \"\"\"\n",
    "# വിക്കിമീഡിയ ഫൗണ്ടേഷന്റെ കീഴിൽ തുളു english ഭാഷയിൽ പ്രവർത്തിക്കുന്ന വിക്കിപീഡിയയാണ് തുളു വിക്കിപീഡിയ. ഇപ്പോൾ 1000-ൽ അധികം ലേഖനങ്ങൾ തുളു വിക്കിപീഡിയയിലുണ്ട്. ഇന്ത്യൻ ഭാഷകളിലെ 23-ആമത്തെ വിക്കിപീഡിയയാണു തുളു വിക്കിപീഡിയ. 8 വർഷത്തെ ഇൻക്യുബേഷനു ശേഷമാണു ഇത് സ്വതന്ത്രമായി പ്രവർത്തനക്ഷമമായത്. 2016-ലെ വിക്കികോൺഫറൻസ് ഇന്ത്യയിൽ വിക്കിമീഡിയ ഫൗണ്ടേഷൻ എക്സിക്യുട്ടീവ് ഡയരക്ടർ കാതറീൻ മെഹർ ആണു തുളു വിക്കിപീഡിയ പ്രഖ്യാപിച്ചത്. 2008 മുതൽ ഇത് ഇൻക്യുബേഷനിൽ ആയിരുന്നു.   തെക്കെപശ്ചിമഘട്ടതദ്ദേശവാസിയായ ഒരു\n",
    "# \"\"\"\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Number of characters in input: {len(input_text)}\")\n",
    "\n",
    "raw_tokens = encode_text(input_text)\n",
    "print(f\"Raw byte tokens: {raw_tokens}\")\n",
    "print(f\"Number of raw tokens: {len(raw_tokens)}\")\n",
    "\n",
    "input_tokens = tokenize(input_text, merges)\n",
    "print(f\"After applying merges - tokens: {input_tokens}\")\n",
    "print(f\"Number of tokens after merges: {len(input_tokens)}\")\n",
    "\n",
    "decoded_text = decode_tokens(input_tokens, merges)\n",
    "print(f\"Decoded text: {decoded_text}\")\n",
    "print(f\"Successful roundtrip: {decoded_text == input_text}\")\n",
    "\n",
    "compression_ratio = calculate_compression_ratio(raw_tokens, input_tokens)\n",
    "print(f\"compression ratio : {compression_ratio}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
