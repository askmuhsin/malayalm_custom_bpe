{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_analyze_chars():\n",
    "    df = pd.read_csv('malayalam_unicode_chars.csv')\n",
    "    \n",
    "    byte_patterns = []\n",
    "    char_to_bytes = {}\n",
    "    \n",
    "    for char in df['Character']:\n",
    "        # Get UTF-8 bytes for each character\n",
    "        byte_seq = list(char.encode('utf-8'))\n",
    "        byte_patterns.append(byte_seq)\n",
    "        char_to_bytes[char] = byte_seq\n",
    "        \n",
    "    return df, byte_patterns, char_to_bytes\n",
    "\n",
    "\n",
    "def get_byte_pair_frequencies(byte_patterns):\n",
    "    pair_freqs = Counter()\n",
    "    \n",
    "    for pattern in byte_patterns:\n",
    "        for i in range(len(pattern) - 1):\n",
    "            pair = (pattern[i], pattern[i + 1])\n",
    "            pair_freqs[pair] += 1\n",
    "            \n",
    "    return pair_freqs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df, byte_patterns, char_to_bytes = load_and_analyze_chars()\n",
    "\n",
    "print(\"Sample of characters and their byte patterns:\")\n",
    "for char, bytes_seq in list(char_to_bytes.items())[:5]:\n",
    "    print(f\"Character: {char}, Bytes: {bytes_seq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_freqs = get_byte_pair_frequencies(byte_patterns)\n",
    "print(\"\\nMost common byte pairs:\")\n",
    "for pair, freq in pair_freqs.most_common(5):\n",
    "    print(f\"Pair: {pair}, Frequency: {freq}\")\n",
    "\n",
    "# Additional analysis for understanding the structure\n",
    "print(\"\\nUnique byte lengths:\")\n",
    "lengths = Counter(len(seq) for seq in byte_patterns)\n",
    "print(lengths)\n",
    "\n",
    "print(\"\\nUnique first bytes:\")\n",
    "first_bytes = Counter(pattern[0] for pattern in byte_patterns)\n",
    "print(sorted(first_bytes.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_byte_pair_merge_rules():\n",
    "    next_token_id = 256\n",
    "    merge_rules = {}\n",
    "    \n",
    "    # First level merges (first two bytes)\n",
    "    first_level_merges = {}\n",
    "    for char, byte_seq in char_to_bytes.items():\n",
    "        first_pair = (byte_seq[0], byte_seq[1])\n",
    "        if first_pair not in first_level_merges:\n",
    "            first_level_merges[first_pair] = next_token_id\n",
    "            merge_rules[first_pair] = next_token_id\n",
    "            next_token_id += 1\n",
    "    \n",
    "    # Second level merges (merged token + last byte)\n",
    "    for char, byte_seq in char_to_bytes.items():\n",
    "        first_pair = (byte_seq[0], byte_seq[1])\n",
    "        first_token = first_level_merges[first_pair]\n",
    "        second_pair = (first_token, byte_seq[2])\n",
    "        merge_rules[second_pair] = next_token_id\n",
    "        next_token_id += 1\n",
    "    \n",
    "    return merge_rules\n",
    "\n",
    "# Test function\n",
    "def encode_with_merges(byte_seq, merge_rules):\n",
    "    tokens = list(byte_seq)\n",
    "    while len(tokens) > 1:\n",
    "        merged = False\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            if pair in merge_rules:\n",
    "                tokens[i:i+2] = [merge_rules[pair]]\n",
    "                merged = True\n",
    "                break\n",
    "        if not merged:\n",
    "            break\n",
    "    return tokens\n",
    "\n",
    "# Let's test it\n",
    "merge_rules = create_byte_pair_merge_rules()\n",
    "\n",
    "# Test a few characters\n",
    "print(\"Testing new encoding:\")\n",
    "for char in list(char_to_bytes.keys())[:5]:\n",
    "    byte_seq = list(char.encode('utf-8'))\n",
    "    tokens = encode_with_merges(byte_seq, merge_rules)\n",
    "    print(f\"Character: {char}, Bytes: {byte_seq}, Final token: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = list(merge_rules.items())\n",
    "print(f\"Number of merges: {len(merges)}\")  # len(merges)\n",
    "\n",
    "merges[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_bpe import tokenize, decode_tokens, encode_text, calculate_compression_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"മലയാളം\"\n",
    "# input_text = \"\"\"\n",
    "# വിക്കിമീഡിയ ഫൗണ്ടേഷന്റെ കീഴിൽ തുളു english ഭാഷയിൽ പ്രവർത്തിക്കുന്ന വിക്കിപീഡിയയാണ് തുളു വിക്കിപീഡിയ. ഇപ്പോൾ 1000-ൽ അധികം ലേഖനങ്ങൾ തുളു വിക്കിപീഡിയയിലുണ്ട്. ഇന്ത്യൻ ഭാഷകളിലെ 23-ആമത്തെ വിക്കിപീഡിയയാണു തുളു വിക്കിപീഡിയ. 8 വർഷത്തെ ഇൻക്യുബേഷനു ശേഷമാണു ഇത് സ്വതന്ത്രമായി പ്രവർത്തനക്ഷമമായത്. 2016-ലെ വിക്കികോൺഫറൻസ് ഇന്ത്യയിൽ വിക്കിമീഡിയ ഫൗണ്ടേഷൻ എക്സിക്യുട്ടീവ് ഡയരക്ടർ കാതറീൻ മെഹർ ആണു തുളു വിക്കിപീഡിയ പ്രഖ്യാപിച്ചത്. 2008 മുതൽ ഇത് ഇൻക്യുബേഷനിൽ ആയിരുന്നു.   തെക്കെപശ്ചിമഘട്ടതദ്ദേശവാസിയായ ഒരു\n",
    "# \"\"\"\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Number of characters in input: {len(input_text)}\")\n",
    "\n",
    "raw_tokens = encode_text(input_text)\n",
    "print(f\"Raw byte tokens: {raw_tokens}\")\n",
    "print(f\"Number of raw tokens: {len(raw_tokens)}\")\n",
    "\n",
    "input_tokens = tokenize(input_text, merges)\n",
    "print(f\"After applying merges - tokens: {input_tokens}\")\n",
    "print(f\"Number of tokens after merges: {len(input_tokens)}\")\n",
    "\n",
    "decoded_text = decode_tokens(input_tokens, merges)\n",
    "print(f\"Decoded text: {decoded_text}\")\n",
    "print(f\"Successful roundtrip: {decoded_text == input_text}\")\n",
    "\n",
    "compression_ratio = calculate_compression_ratio(raw_tokens, input_tokens)\n",
    "print(f\"compression ratio : {compression_ratio}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
