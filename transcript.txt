ERA V3 Session 11 Studio - YouTube
https://www.youtube.com/watch?v=rEqxkAUwsVo

Transcript:
(00:01) all right so welcome to today's session today's session is going to be our part one on how do we train Transformers from scratch uh there's a bit of uh cabus reshuffle again you can refer to the cabus we have pulled in some sessions to make sure that we have a direct start on the LMS in instead of actually looking at small LMS and some of the code for that so this way are going to get really good idea of how LMS are going to perform and how do we train them from scratch and as what are pitfalls and
(00:31) what are the things that are involved and this I believe uh was around 20th session in the era and before that we had around five sessions just talking about LMS but until this session and the session afterwards people were still confused so I thought like we actually wasted five sessions so that's why we pulled in this session early so uh this is a uh two-part series actually and heavily inspired from and gpa's video so there are some links for that video also so going we get time look at that but this is in our context this is in the
(00:58) context where uh here assumes that people know a lot more compared to where they are so we have tored it down slightly and we also made sure that we get to understand from Indian context because his BP and other things are mostly speaking about English and other things and we're going to look at how when we say that we're making our LS for indic languages or uh things that are relevant to India then how do we actually take care of that or for any other field for that matter okay so the core topics today
(01:28) that we're going to cover first is going to going to be how do we understand tokenization second is going to be what is Pair by pair encoding then the implementation of VP from scratch partial coding demonstration there's a lot of code that we're going to look at today then we have integration with GPT models and uh finally it's going to be testing on validation side that how do we make sure that we are covering some cor cases and our tonation actually takes care of all the things that we interested in now I I've shared this
(01:55) earlier also in case of images uh we know that we have textures and patterns and parts of objects and objects and uh textures uh edges and gradient is something that we have to make kernels for and we have to identify them and we are saying that we are okay with 64 or 128 and some of those and U there's no there's no International definition of or a collective definition of these are the only edges that I'm going to work with but we have such a clause in language we have words in languages where we say that car is going
(02:23) to mean this and then aure is going to mean this and then biology is going to mean this and for every word we have different meanings also depending on the context we using it so we have this restriction in language and a benefit also because not things are defined and we can start from there but how do we take those words and give it to llm is a big question and that is something that still uh I believe not solved properly because yesterday while sleeping I came up with an idea and I thought actually this cool idea and I'll think that uh
(02:50) I'll try and Implement that as well and there are a lot of work that is happening onization front today's session is going to give give you a really good idea why toiz is the first thing that we need to understand even before we start approaching llms okay now uh if you do the session this session and the next session this is a course reset by the way last session we got some idea on uh what LMS are but this session and the next session course reset so whatever we have done till now uh apart from pyos python
(03:16) some Concepts on reset and uh training and ec2 and front end back end and other stuff everything is going to start from scratch so basically except convolution everything is usable and uh we can use the same Concepts from what we are going to start from today so if you have not been focusing till now then this is the session to start focusing again because comets we're going to be touching upon very um briefly again in some some uh time to come because whatever Comet can do our uh llms can also do that all
(03:46) right so let's get started so as I said this is heavily inspired from and K's videos do look at them they are really really amazing but I found these two interesting links for you the first thing is called killed by llm uh this is the death that has uh being caused by llms to some of the benchmarks that are there now AR AGI uh came in 2019 till 2024 was recently killed by O3 so human is somewh around 80% but o03 it hit 87.
(04:17) 5% math benchmarks these are the benchmarks that are recent in fact let's start with the old ones so swag was very uh very interesting data said B killed it then we had Squad V2 and Squad and trivia all of them are being killed by llm so whenever we are coming up with any new data set LMS just make sure that those data sets are being breached their accuracy compared to humans for example Haag was a really important one that was also beaten then we have MML vinograd all of these are the old ones that we're not going to be touching but Turing test
(04:46) for example it took a lot of time 73 years but finally integrator at 46% beat this also then human evalve is something that is on the coding side that is also now gb4 uh really really good humans are somewhere around 83% in there now R KGI was the latest one where uh it has hit 87.
(05:05) 3% so that's really interesting to know uh so keep uh so this link is there so you can look at it in time but as more and more benchmarks are coming in these LMS are actually beating them so that's really good then second is this beautiful AI timeline built by hugging space and it talks about models that are continuously being launched so we have in December itself we have Nova Sora o1 and O2 Pro and we can look at uh open weights only so gr knite V4 then llama then deeps V3 this is causing a lot of uh uh I say controversy as well as uh PR
(05:38) for them because they actually reverse engineer1 and then try to understand how can they train their own o and uh it's actually pretty close to own as well and it's open source you can download it and you can run it on your laptop desktop if you have really good GPU but you can run a quanti version as well so there something to keep in mind that you have these links where you can look at what are the models that are coming coming in and uh I also found one interesting link and I'm sorry I found it late I found it
(06:06) at 1:00 a.m. last night and there is going to be this link now this is called sefat and air bench and I wanted to look at this it's strain on anyways 800 so we can't compare it but if you just look at the flops they have killed it and there's lot to learn from this air tension I'm going to take their uh Nano GPT approach also and introduce you to intr introduce you to that because there's lot that is happening there right from how the positionings are calculated to a new form of uh uh Optimizer whereas Optimizer as well as
(06:49) uh activation function so there is an update on activation function interestingly and uh there are a lot of good work so we going to look at that but if you were using this then probably your reset uh uh exercise assignment 9 you would have actually traded for far more numic box but we need to take a look at it and interesting thing that they actually claimed and there's a paper also now we have heard about uh random flip right uh we take an image and randomly flip the images uh they're not doing it they they are doing a
(07:17) consistent flip so if you see the car is in this direction then this direction then this direction and they're actually get a pretty good accuracy Improvement just by using this this activation uh this image augmentation I thought I should raas this with you and and there are other things also uh that I'm going to take a look at it but this guy is kidding it he has one more repo called moded that's correct so mod Nano GPT and again inspired from andati and when he launched it uh he wrote LC Baseline it
(07:48) took 45 minutes anyone want to guess where we are today on this PR can you guess what is the record now yes Rohan I I'm here with one request Rohan if possible can you include something with Lang fuse also Lang fuse talking agents now yeah yeah that's pretty late in the course we I'm actually starting a new course just on agents we have one session on agents agents require their own full-fledged course but I ask you a question here we have 45 minutes for the record training n GPT what do you think is a new record
(08:30) minutes lucky number three 3.57 minutes that's like insane and let's see what is changing it so they're doing a bit of architectural modelization than meon Optimizer you can see the literally like 50% drop in a training meon is really interesting and I don't know how it works as of now uh looks complicated and there's a lot of uh math involved then meon implements then just upgrading bych give 1% more but you see from meon car also you still have me on so I'll start let's say from 31 so one3 time 2/3
(09:05) of reduction then shortcuts and tweaks then unit and toxr flex attention also is something that they added then we have uh yeah this is a good description so and and these guys they keep interacting with each other so that's really good so this is what has happened and I would like to like you to take a look at what they're talking about rary embeddings we are going to talk about it then we have something called KQ Norm uh last time we discussed that we have a Q Vector we have a v vector and we have K Vector all
(09:34) of these vectors are literally are same vectors right and after that we send them to our neur Network to become a new k- q- and V Dash we do k q transpose and then multiply uh of course we have some normalization thing also Dimension square root and then we multiply by V right V Dash that's what we discuss but what they're saying now is that uh this guy it's better that we actually normalize them to make sure that the overall sum of their amplitude is equal to 1 and that's that is what K we are
(10:02) going to go through each one of them so don't worry what do you think this is how will you read this really squared squared so so we have x equal to Z for anything that is uh uh less than zero right and we have X = to X if our X is greater than equal to Z but in this case we're going to say x equal to x² and that seems to have added a good amount of improvement then we have Monon uh Optimizer we're going to look at it what this is then we have a unit head from embedding untight head from embedding
(10:41) and you do not know about this you're going to learn what this exactly is in the next session and uh then we're going to say that is what and is also going to explain that what is it that we do we are tying the head with the embedding basically I'll explain this in next session uh take to most time then we have projection classification layer initialize to zero that's interesting then we have some architectural shortcuts resal and embedding shortcuts that are there momentum uh warm up then we have tan at soft logic capping uh
(11:12) this is taken from Gemma 2 that has come from uh the Google as of now then we have Flex attention and we have extrating Fed so a lot of work that is happening and take keep an eye on this reports very very interesting and the way they are working I think uh people like us can actually train more and better so good guy to look at but now let's go back to our own boring session so let's start start on tokenization now what is tokenization and what exactly are we're going to be covering today okay now tokenization is a process
(11:43) I'm going to read the text in front of you because you sleepy tokenization is a process of breaking down text into smaller units called tokens we do not send the words as is we do not send the character as if we send something in between and we're going to see why actually these tokens Can Be words it can be characters or subwords but we are not not going to decide it's going to be decided by an algorithm that we're going to write this algorithm is called BP now depending on the application and
(12:07) specific requirement at of the task at hand and we need to feed this bpe with certain rules based on which language we're targeting and that's where the catches because these rules are for English and if you use the same uh rules for Hindi or Canada or malal or Telo or Tamil or other language that are there it's not going to work lonization help convert text into structured form uh that can be mostly processed by algorithms M model so we're going to see what all of this is but let's see what
(12:38) uh why it is actually required why tokenization is required at first place compression now natural language text can have a very large vocabulary so think of it like this how many words do you think uh Donald Trump has in his vocabulary I believe 500 he uses 500 words in combination of that to speak and explain everything yes how many words Shashi has I think he has like 5 million words right and if you go to Oxford dictionary uh that may actually have around 20 million words because they will include words from other languages
(13:16) also how many words are there total in all the languages what do you think eil how many words are there in all the languages combined don't explain just let's see what says 7.5 million distinct words okay so more than um just close to shash so I was wrong on ox but you get the idea a lot of words okay now what it means is if he work at the word level and we say that we want to Target every single language then our Transformer must output one of the 7.
(14:08) 5 million words do you agree and this is just language we need to look at the programming languages also right in programming languages I can also write my funk and I'm sure my funk is not a word in uh Oxford and those are 7.5 million root words thank you got it this is not feasible agree okay now the second approach we have uh is that we can go with characters only now if you have characters then maybe uh 65 characters are going to help us Target all the uh English languages but if you expanded at 256 then we can include some German and
(14:50) other European and Russian I don't know but uh let's write it there but if it includes some 2100 characters then we can include literally all the languages in the world and I'm saying all that includes uh English Hindi uh Hebrew and German and uh Chinese and others also this is like we're going to see where this number coming from in that case what we can do is we can have a network that predicts only 21100 or maybe 2500 tokens 2 2500 characters and we can cover all the languages good but now the
(15:22) problem is the input is going to be very long because let's say I just send this I am Rohan okay so we have 1 2 3 4 5 6 7 8 9 10 11 11 inputs here H not a problem but what if I say I am unal don't talk to me under the spelling okay I not show an so now we have 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 400 right or what if I say I am Mutu Krishna chin Swami Guru Swami venkata Ragan and so on and so on now in that case you can see that we we getting into trouble so we can't work with characters
(16:19) we can't work with words we work with characters the input becomes really big if we work with words the output becomes very big so we need something in between and that's the compression we're talking about we're talking about compressing sort of both at the same time some sort some sort of a midline where uh we can handle the input and output both at the same time now if you read what I'm saying here also uh natural language text can have a very large vocabulary as we see 7.5 million root words and more
(16:43) on the programming languages and when multi lingual models are to be trained we need a lot of words to take care of the simplest tokenization consists of characters that can build from 26 characters plus 10 for numbers and X tokens for English which are going to be very few but we are talking about multil language and multilingual and uh exclamation marks and emojis and whatnot and remember even a large LMS they have a tokenization they have vocabulary size of 50k when we say an llm as a vocabulary size what is it that we
(17:12) talking what is the vocabulary of character siut to this is is this equal to words no no this is not equal to words is this equal to characters it is not equal to characters it is something in between as we are discussing and those things are called tokens are we good yeah any question on this no to all right now the second thing is this part where it simplifies the text processing it simplifies the NLP text complexity into manageable units something that we can can convert into tokens and that's going to be super
(18:01) useful for us because they're going to be word that we can't understand uh programming language Snippets and uh the way we write python versus C+ also very different indentation on all of that things that are really not there in language right for example if I use more words between more time between words I'm not changing the meaning of the words sometimes you're talking to why you do but uh here in general context we don't but in case of python the amount of space we have between the starting of the word and
(18:30) your left side of the screen it can mean something else it converts text into standard form that can be processed by models and then we can if all the models are using the same tokenization then we can actually cross feed at it also which is also pretty good now this token also becomes a fundamental unit of analysis we can look at a token and understand what is it trying to mean or what is the uh pattern that LM is trying to learn based on this particular token and how do we make prediction also so we can make the prediction that also so we
(18:58) convert the languages into to token and we predict those tokens also so it becomes a singular unit that we are learning and we predicting also so it becomes slightly easy then we have text normalization uh so here you're seeing two examples one is called stemming other is called limiti something that we do not use but just for understanding when we say litiz we looking at all the words and looking at a root word which is change here but stemming is something that we going to be using in our BP also we're using change changing changes
(19:25) change changer and so on gets converted into chat and after that we need to somehow attach these to this guy and second then we have it improves model performance uh because now model can capture fine grain in patterns as well it can understand what is Chang and changer and what is CH Chang and in so it understand those things and what how does it change the meaning of a particular thing and server tokenization method can help out of vocabulary words also right for example if we say that if you say that if you go to a gr
(20:04) class then you are through right so now I just made this word so T and then we have Ed this word is not going to be in Oxford but nowly we're given an option to the language or to the tokenizer to break this up in such a way that our model can understand what is the meaning of this thing called th right then we have support language understanding it provides a way to analyze text Contex actually from the basic unit sort of your root words to a meaning in a sentence and it also helps understand syntactic structure and syntactic
(20:37) meaning of text which is very important for passing and semantic things and the best part is that it is helping in storage and retrieval because here we're talking about compression and you're going to see what is this compression and uh the assignment for you is also to compress a language and make your own VP for a specific language so it allows us to index documents efficiently allowing fast and accurate Ral and so on so a lot of benefits basically just on that but the most important part is that it
(21:00) allows us to handle multilingual text simultaneously in the same model and that's where the benefit of terization comes in okay now there are many kinds of tokenization uh we have word tokenization and we discuss word tokenization is going to be not useful for us but here's how it's going to work we have tokenization is fun as a language as a sentence coming in we we divide tokenization is fun and exclamation mark into four different words this cool then we have character tokenization where the token becomes 1 2 3 4 five
(21:34) tokens so here our three words become four uh here four four so Word level then we have sub word this is what we're going to be looking at and we're going to figure out a strategy of doing this automatically for us here we have we're going to be splitting our text into sub words of smaller units and smaller units then Words which can be useful in handling unknown words or or Rich languages like Sans also for example the word unhappiness can be divided into un and happiness but someone has to decide that
(22:02) un is going to be together then happiness is going to be together and so on and you're going to see how this is done and uh what is the use of something like this but tokenization is a very very complex thing to solve and the reason is we have a lot of ambiguity determining the boundary of token in a word can be challenging uh because we have languages where things are drawn for example here is a word in Chinese here is another word in Chinese and both of them have a very different meaning right and here is another word in
(22:30) Chinese so and this also looks similar to our a in Hindi also right so a lot of ambiguity in how do we actually decide them this is not a confusion confusion is that how do we break these Sy symbolic languages then we have context sensitivity for example if I say I saw a New Delhi this time right this has a different meaning compared to I s New Delhi now should this be one token or new and Delhi should always be separate right I saw a New York so if you say New York you know that new and York always come together but if I
(23:16) you're talking about a city called York and you're seeing the city was transformed because of the cleaning L and then you saw a New York in that case it has it has a different meaning so contact sensitivity is also something there this is not solved in yet as of now then we have special characters we have pations we have emojis we have special characters and then people keep making different symbols and different combinations of asky to come up with a new a description or something so we need to take care of those also so it's
(23:40) not easy but we have to figure out so here are four main algorithms first first is called whites space tokenization where we just look at the whes space and we uh use that to divide a word then we have regular expression we're going to be looking at that and here we're going to be using a reject to split our sentences into uh something that is manageable then we have NLP libraries this is all of these died in 2016 to 2018 kind of time frame I'll say 2020 and some of them were surviving on ventilator we had libraries
(24:13) like nlk nltk space and hugging space Transformer which itself is actually just BP which provid US tokenization metod also so we can use them as well and finally we have something called bite pair encoding and this is one that is now ruling the world now B encoding is something that uh was something that you can see till gpt3 or probably four also but after that people have started hiding exactly what they're doing with their bite PA en coding because um this really had a big effect on this like a DNA right so bite PA
(24:44) encoding algorithm is going to decide how your text is sent to the llm and how llm is going to represent that text in its output so it's really a a secret source for most of the companies they're not really open about how they are attacking this but if you're Chinese then you can reverse engineer but you're not that means I have to train it teach it to you okay so the server toiz method that ially emerges most frequent pairs of byes text so we going to look at that okay so let's look at the Character level to conization
(25:12) first so let's go here uh and uh first do not read this okay first let me explain what it is and then they're going to come back here and say yeah it is R so I need to this log in once okay so now let's start run some coding I'll keep myself on side the first code I'm going to show you is going to be I we do not need uh GPU so let's leave this guy behind first code I'm going to show it to you it's this one what is this code doing immediately fast please quick too slow styling for the output yes I'm
(26:03) adding styling for output because things we going to look at they are very long and I don't want to scroll on the right hand side of screen I want them to fall just one after the other so now if I write anything for example if I write print hello not sure it's working properly no it's not working properly the output it's for the output the output you can see is in helvetica correct okay so that's all that's all this code is for I just wanted to explain why I'm starting with this and
(26:31) now let's look at our character level encoding okay now we're going to uh use this data set which is called Shakespeare and this it's going to be stored as a input. su file and let's see what is this going to be saved as Cur yes so this is the file that we have and this file has text this text is in English and written by Shakespeare and uh we're going to be using it okay now first let's read it and uh see what this is all about in the code because if we do things in code it's it feels like
(27:26) we're gigs right so we're going to take all the text and we're going to put all of that into this variable called text now let's look at how many characters do we have in the text which is going to be the length of text inside and that's going to be around 1.11 million okay so we're working with this large small text compared to who we talking about and let's look at the first 100 characters that are there and these are going to be first citizen before we proceed any further hear me speak all speak speak
(27:53) First Citizen you're all resolved rather than rather to die than to famish all resolve resolve and so on going to read all but that's what Shakespeare is doing when he didn't have time to spend on Instagram okay let's find some unique characters and how do we find unique characters we're going to be using R set so if you take this text send it to text send it to set create a list of it and then sort it out you're going to get all the unique characters yes okay now let's print those GS also to
(28:34) see uh this is exactly what I wanted to avoid so maybe I do run my block again yes I don't want vertica I just want my things to be not falling vertically still one second e still suffing okay live with me then we have back we have back sln which is basically this enter thing that is
(29:37) causing the next line to come then we have space exclamation and we have percentage sign and dollar and so on but why are they coming before ABC and D any idea ask in asky they have a value Which is higher than the other characters cool okay now let's look at a full code and the full code for us is going to be we take our text we put it a set that gives us unique characters we convert that into list we sort it out and we calculate the cap size and we take a empty it's not a space I can't say empty
(30:20) space we take a empty string and join all the character one by one so we can actually get this in a single line yes so finally we can see that this is our text we have CD and all characters and bigs and total 65 characters that are there you're not seeing any numbers there some three was used some in some place is it good okay now we would like to create our character level encoder so now we're going to convert our strings to integer and this is called s oi don't forget it okay we we writing something to convert
(30:53) our strings into integers are we good okay before I even run this uh do you know that uh you can do something like result equals to a list of I for I in range 10 you know this or no list comprehension a list comprehension right similarly I can convert this into a directory also but for directory I need to provide a key and a value right R it's a dictionary dictionary sorry correct okay so now if you understand this then let's write our strings to integer and this is going to be character and it's index
(31:47) from or index and character that we can get from our CS but cars are just C so I need to them you clear I have to do a mistake H oi so we have this dictionary which is going to be storing all the numbers there good now let's do the reverse Also let's do integers to Strings and now this is going to be our index followed by the character for I comma CH in enate Gs now I have a reverse of that good okay now let's write this encode function so encode is a Lambda I don't want to write a full function where s is
(32:42) going to be the input that's going to be the output and we're going to write it in a list comprehension so St oi so convert the string into character given this character for the character in a string given to it we good similarly we can write a the code that is going to be Lambda of a list of characters a list of index given to it where we're going to join the result for all the characters sent to idos all the integers send to itos where this index is being R read from a list we good okay now I will just copy paste all
(33:36) the code so that fix all my mistakes okay so here we have a string to integer here we have a integer to string here we encoding and here we decoding I'm going to send encode High there I'm going to get some result and if I encode High there I'm going to get integers and if I take those integers and then decode them then I will get my string hi so working as expected clear all right now uh we are in AI class so we have to import torch we do not import torch torch gets angry so let's import torch immediately so he
(34:18) doesn't feel that we are leaving him behind so we have imported torch now we're going to take our data and we're going to take TCH and we are going to also call to's friend called tensor tensor is like th he gets very angry if you don't call it call it then we going to encode our text so full text is now encoded and here we're going to be using data type and that is going to be tor.
(34:45) long long is essentially int format that is there in int 64 that is there in pyos and we going to be using that we could use use other integers also I'm not sure how many characters we have we have right now 65 so can have we could have used something small but long is something we are going to be using because in future we going to look at 50,000 and others also are we good then we're going to be printing our data do shape to see how many uh what's the size of a data set we're looking at and we're looking at data DOT type B
(35:15) type that's going to come back as not long but N64 just to prove it to you and we're going to just print our data of everything starting from 0 to th000 hoping this quote is clear because I'm going to share this file with you I will let's run this import to is going to take a bit of time it's getting ready putting some makeup up then looking at the mirror and not it's ready okay so we have imported our tou and then we are seeing that we have 1.
(35:44) 11 million stuff in 64 and this is the encoded text that we gave it for the first, one happy good all right so we're getting closer now what we're going to do we're going to take our data and we're going to say that uh touch do tensor why don't you please encode hello world for me exclamation mark also and don't forget that we have to say that data type for you is going to be tch.
(36:18) long and uh we're just going to print some stuff here and that's going to be data do shape and D type do shape this is being done just to make sure that we see what is happening for us small world hello world gets converted into 1 2 3 4 5 6 7 8 9 10 11 12 things is the white space Also encoded answer is yes this is your white space which is here and if you go back here you're going to see no you can't see now because you combine it here you can see character space all right okay so we have just done
(36:55) character Del encoding and uh you can see that we have a lot of things that we need to look at now let's go back to our notes but this is two rudimentary and doesn't lead to any compession as well hello word is converted into 12 tokens instead of possible three 1 two and three right and that's the uh reason that we need to go back to something else that we can actually be using and that's called bite pair en coding and we're going to see why now bpe was not invented by gpt2 GPD use BP but it was we need to look at this paper
(37:31) it was introduced in this paper in 2015 for using NLP but it was also not invented by in this paper it was actually invented by Philip Gage in 1994 but this guy was poor he didn't have Nvidia gpus or money this guy had some money they had a lot of money right so when people tell you money is not everything uh in life you know who says that people who have a lot of money so if somebody has a lot of money they're going to say money is not everything because they know then that even if they have money they can't buy everything
(38:12) sometimes you need connections so this PDF is gone and this link was gp2 link yes so we'll find gpt2 paper online gpt2 r five we need to look at that a lot so yeah okay so this was the paper uh relas in 2016 2015 is what we saw there but here they spoke about neural machine trans world and so on and if you search for contr f VP then we should find by pering so we adapt by per uh encoding by gauge so forget this paper let's go back to gauge if you write your text like this and introduce your paper like this
(38:54) without photos or images or some graphs then people people will not be interested like we our generation is all about looking at beautiful images graphs and generated stuff if you write like this people are not going to adopt so that's probably the reason why I picked up but the actual algorithm came from this guy and really really amazing algorithm that uh was used in compression actually so was actually used a lot and after that people realize that okay we can actually use this in our things also and that's where we
(39:21) have just gp2 it's not gp2 paper I think this is the paper yes we're going to refer to this paper a lot okay let's go back so the algorithm is defined in just one sentence and that's a beautiful part bite pair encoding is a simple data compression technique that this is algorithm itally replaces the most frequent pair of bytes in a sequence with a simple with a single unused bite this is the algorithm clear if you all all of you were uh GPT 401 or 03 we could have ended the session today now but we all are not so we have to now
(40:12) go through what the hell is this so from uh gpt2 paper uh BP is a practical Middle Ground between character and a Word level language model which effectively interpolates between the word level inputs of frequent symbols and character level inputs for infent symbols and but let's focus only on this part ially replaces the most common pair of bytes in the sequence with a single unused bite okay so now let's define our some text a a a b b a b a a a c b and a how many uh independent characters do we have
(40:51) here three three we have three so a let's say we give it a bite location one B we give it a bite location 2 C we give it a bite location three all right yes no okay then we combine look at the two consecutive words consecutive characters at once so we have a a followed by a a followed by a followed by BB followed by B A followed by a followed by B A followed by a a followed by a a followed by a c followed by CB and followed by ba a good we sliding window now look now we now count the pair that has occurred a lot
(41:37) so we have 1 2 3 four times a a coming in and then BB has come only once here let me add one more B here so this becomes BB comma BB and BB has come twice here all right so a a is going to be our new token we write the token here and we give it the four unused bite and then we simplify this now this is going to become AA we need to give some uh name also because AA is a very long bad word so let's call it X so we have x a b b b a b x a c b and a clear okay so now again if we do the same exercise we going to realize that
(42:27) we have sort of compressed this sort of compressed this now we have BB coming twice and we have AB here and we have AB here but AB came twice uh in the dictionary before BB so we going to convert AB now so AB is going to be given the bite number five and let's call this y are we good so we going to get x y b and b and a again converted into y then we have X then we have a c and b and a so what you're seeing right now is that initially we had 1 2 3 4 5 6 7 8 9 10 11 12 13 14 we needed 14 tokens to represent art
(43:10) information but we only needed three Global tokens let's call this vocabulary we needed only three tokens in our vocabulary to represent our information but that needed 14 tokens now we have increased that to five tokens but now my Tex can be represented in 1 2 3 4 5 6 7 8 9 and 10 so seeing this is reducing and this is increasing are we clear okay now uh there's a really good example of this on Wikipedia we're going to look at that example because it's going to help us explain a lot but BP is really good
(43:51) but BP is BP has its own disadvantages also and I'm going to run a video where and is going to explain this because he's using older gp4 where all of this is possible to be seen and he's also using at a API level where you can see more flaws from gp4 because uh that is kept cleaner compared to the highly restricted gbd4 that we use on a Char GPD now llms can't spell words properly and when I'm reading this please believe that we are in 2024 of July literally 6 months ago LS can't spell words properly
(44:24) they cannot do super simple string processing like reversing a string uh they are very bad at words that are not in English they can cannot do simple arithmatic uh gpd2 have more necessary Double Trouble coding python also we're going to look at that my LM abruptly halted when it saw a string called end of the string this was not added in the code uh the weird warning I get when I'm using a trailing SPAC this happens when you use API again remember July 2024 then why should I prefer yaml over Json LMS again problem with tokenization why
(44:54) LM not actually NN language it's n2n toon ation language and the root cause of all the suffering is actually tonation and we going to realize that that's actually true now we have not trained our LM so we have not seen all of these issues so I'm going to show you a small video we can look at all of them but the problem is that uh tokenization is the heart of everything and we need to be really really good at it now let's look at GPT 4 and gpt2 tokenizer first on this web app we're going to copy
(45:18) paste some text from the collap file and I'm going to paste that text here also so you can also do their experimentation and this text is specifically written to WR for this exercise so I request you to copy paste this and I'm going to add a text block we have some Hindi and some other stuff but we're going to look at this thing called tick tooner why is it called Tik tokenizer Tik token is the name of the library Tik to is the name of the library why is it called tick token author wanted it so the author descided
(46:01) to call T token that's all okay so we have this uh text here and here we using gpt2 so first of all look at a token count here and before we do that I'm just going to copy this exit here go out and uh word on and there's some application word contract thank you and I'm going to go there and paste it and we're going to see we have 101 words so start with that our tooner has literally doubled it 2.
(46:41) 88 okay and what I want you to notice is first of all we in gpt2 tokenization tokenization itself is broken into different word we have token and isation you can understand that I want you to notice few interesting things and uh once you notice them you're going to realize maybe after some time not immediately that that actually makes sense do you see that is has a space in front and this whole thing is one token same goes with at also where this is one token same goes for the also where this is one token so the trailing space is made a part of a
(47:13) word can you see that okay can you see that llm is divided into two parts where we have LL and M's correct yes okay can you see that 127 is together but not 677 773 is together but 77 is not 804 is not where it's very close to 773 and 041 is which has three things but for some reason uh 804 and 75 and 12 are also to separate now the good thing about uh this app is that it has something on the bottom everything should have some bottom but I want to notice at eggs why eggs because you're are hungry and to do that something I should have
(48:16) done in my last course also because I kept struggling how do I show my eggs not my eggs like these eggs I hope you get the idea not my eggs okay okay so now here let's look at these eggs so look at the first egg the first egg here this guy egg is not a single token got it full stop is 13 then we have this 198 which you are seeing and now suddenly it comes back this 198 is Mr India got it trailing space the uh new line new line character that is there but not visible so this is 198 so this egg is separate then this egg is together what's the
(49:08) difference this egg and this egg this egg has a space before it this egg does not so if I go there and add a space then certainly that's a single token and that that token is 14562 this egg is 14562 but if I remove that space that's a different egg right eggs can be different right you can be looking at quail egg chicken egg dinosaur egg T egg so all of these eggs are different similarly the small character egg is also different that is 33852 and then this Capital egg is also different we have EG together and then G
(49:42) together and then this egg is also different because it has a question mark at the end so you see this uh uh behavior for one egg we have so many different tokens and should we have these different tokens for eggs or not is something that only a chicken can tell but problem is we don't know what came before chicken or egg so that question is something that goes back to you now that you have to decide that you are going to be treating all these eggs separately or all the eggs are same for you right so that's something that we
(50:09) have to not take a call at but you understand the problem that's coming here now I'm going to go back and paste the rest uh text for me and that text is here so I'm going contr AC and go back lonizer and then control V8 now we have looked at this so I'm going to remove that then we have looked at the numbers I'm going to remove that I'm going to remove all the eggs and we only going to focus on Hindi for a while I'm going to remove the code also because we're going to be bad moing
(50:40) that python code later on now look what is happening here Google translate but you can see that every single part is not together these words are not together because gbt2 was not made interestingly gbt2 was made by a lot of Indians but they don't focus on the Indian language they focus on the English language because that's where the money is coming from so this is a problem look at this sentence here we have 1 2 3 4 5 6 7 8 9 10 11 and probably 10 spaces and so overall 22 best possible scenario for tokens to
(51:14) be sent to an llm but how many tokens do we have here we have 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 so around 85 Maybe so look what is happening the problem is that now what is going to happen is that if you send Hindi text right let's say when a 50w sentence to token to LM internally it is going to have a massive attention block because the tokens are going to be converted like 400 right so and if we have to look at 400 words 400 tokens and then we have to identify which tokens links to what so you'll
(51:50) start understanding this problems you understand attention mechanism in case of attention mechanism I need to understand myself with respect to some other tokens right we not sending words anymore and other languages apart from English are not represented even today as well as English is represented and this is the problem that we're going to face now anyone uh when you go online on LinkedIn anyone claims that they have written uh LM for inding languages just try and ask yourself have they work on the tonation problem as such or not if
(52:16) they have not then I have a very very beautiful image for such situations and as I save it and I show it to people a lot and I hope you also like this image for all those people who claim that they have trained their models on inding languages but they do not work on BP this is what I sent have you seen this image actually I should store it so I should store it as Pakistani good okay so that's a problem then uh doing something like this now let's go back to our code and copy all of this go back to tokenizer so we just
(53:06) bad mouth it people not using Hindi I'm sorry I'm in Karnataka I should have copy pasted Canada uh do not th me out from bog okay now here we have code in the code you're going to see that spaces spaces spaces spaces spaces all of those spaces are like different spaces and that's a problem here because in case of python we have to Club all these spaces together and that means something the indentation has meaning in Python some other languages it may not be like JavaScript or C++ but in Python that's
(53:34) everything now interestingly you can see that spaces were taken care by uh the bpe for the GPD but in the code when they saw so much space it collapsed it so gpt2 was specifically very bad for python because they did not take care of the spaces in the code that in the tokenization that was sent by BP it was taken care but while sending it to the model they remove all that space bit one and when we do that py is not going to to perform as well but you can see that how many tokens are there just to represent the code also right Fizz bird
(54:02) and print and all of that now has a token which means that the one the 7.5 million words that we looked at assuming that they had all the words also now we looking at programming languages where all of this has to be treated very differently so that's the problem with tokenization and now you understand how important it is right so if we look at all of them at the same time for 101 uh words look at the number of tokens that we're looking at are we good okay now we're going to go to CL 100K base now CL 100K base has reduced the
(54:38) token con to 212 and earlier we were at gpt2 282 now we're going to go to this is rp50 and others and others let's look at uh gp4 oh something else all together I'll go back to this only and I'm going to Bas myx there the CLK CL 100K base was used in gbd4 and three so I written it down gbd4 tokenizer so here the token count is less and we have a lot of other things now coming together especially on the Hindi level look at that W is one word now right they're training on more and more indic data now as you start
(55:16) training on more data that is of other languages your PP is going to say is going to find those repeating uh characters together a lot more number of times and it probably might advance in case of calculation that how many times I've found that pair now you understand where this is coming from let's say we have English we have German we have Hindi and then we have python now in English we had let's say one billion characters and in German data set we had 1 million in Hindi data set we had let's say 100K and in Python
(55:48) we had let's say 100,000 only th000 only okay now in this case a lot of characters are going to come together a lot of time so in that algorithm where we are combining two characters and counting the number of times English is going to win so all the characters in English are going to win then German then Hindi and then probably python right so that's the problem that I'm talking about the amount of data set we have for a particular language is going to rule the tokenization and we're going to see that when you look at the
(56:11) tonation as as well here now we have again eggs are different uh and here eggs are also different question marks also different because the base is not changed we going to look at how they do it but at least the law of Hindi is also taken care and now suddenly you have specific spacing for python right you can see that whole of that thing is given one space all of that thing is given one space so now you started taking care of the programming languages any questions till now is it possible for to merge B language SE
(56:42) train yes answer is yes any questions till BP on BP till now no question yes thank you ROV uh so what you was saying is like the GPT has not seen other languages that's why this BP leads to this kind of um tokenization so if had not seen lot of languages that's why you were seeing so this look at just this V okay I'm going to now move to gpt2 and then V is now divided into two part which means GPT 4 had seen a lot of Hindi stuff compared to gpt2 okay so they they sced the internet to collect the data it's just that they
(57:37) filtered out the cover enough of Hindi data so gbt2 is before gpt3 we're talking about like dinosaur era so in that time uh the focus was English data set only people started now making data sets Okay now this is it also gives you how many tokens we have here we have 100K tokens here now let me move to uh 200k Bas and suddenly you're going to see anal also becomes one word right so we are increasing the vocabulary size when we increase the vocabulary size the less frequent things are also going to come
(58:06) together a lot that is what is happening here okay okay sham here on so if for any language if it's going to be a numeric representation call it a tokenization uh how how different languages would play into like if it's going to be represented into a number like every character of it so the repeating pattern could also be identified regardless if it understand that language or not so question so like why uh tokenizing a different language is difficult here like why this swaka is one in uh o200 K and not in C 100 oh it did not occur
(58:50) admin times to be counted okay let me explain by EX example so let's say we have a a b b a a b c b c a b a d a d and uh d e and d e and now let me write MN okay now so a b c d not D AB b c h d also and then we have M and N so bite one bite two bite three bite four bite five bite six got it let's say our Target is that we can only have eight bytes so let's see what's going to happen we our AA is going to convert into by 7even because A and A A is twice here so that becomes our x b x bc BC then ad a ad d e d e and m&n now now
(59:56) if you count next you're going to see BC VC is twice so we are going to convert BC into 8 and let's say x b x y y a d a d d e d e m n and we're done I can't now put this into my tokenization because I give a limit of eight so when you start your BP algorithm you have to say how many tokens you want to end up with 50,000 like gpt2 100k that we saw 200k that we saw in the next one where more words were there uh got it so uh so in that case if our entire text if the word walk is repeated the most number of time that could be a
(1:00:36) token in itself that could become a token may become a token yes it has to cross cross okay got it thanks okay R so uh for example the word tokenization or weirdness that we see that those are broken down is it because of something like uh in the earlier NLP classical NLP lization or something it has nothing to Li you're going to see we going to write a code what is going to happen is the token word is going to find a lot many times compared to tokenization oh okay okay I got it okay no I'm not done yet yes or
(1:01:16) no yeah y yeah yeah y so that has already been converted into some bite so we have a bite isation then we have civilization then we have minimization so all of those isation is coming from other words also that's that's going to be club together in a separate entity so this isation has nothing to do with stoken it has to do with isation minimization maximization and frtiz and terization and so on okay okay Arun ra so uh you have written like Google Translate in Hindi yeah but uh it is also taking care like how did that do
(1:01:54) like how does it know like this is English word like I mean does it really matter at all I didn't get the question how does it know that Hindi is there it runs translate is there right in Hindi you have written Google translate cor so translate is in English right it that the Reas BP doesn't know anything BP is pure statistical algorithm yeah that's it okay okay mahavir yeah so for when we you for the examples you showing um you go uh like kind of a fall Loop Right Where You Keep On adding um that next the next
(1:02:33) count word as as token right so is it always in in this increasing order of that you always yes do it this way that you find the most frequent word yes in BP yes so could it be like for example I was just thinking maybe it would be covered for example um uh there could be Words which are like for example uh token token for our class or whatever we are talking about token is repeated so many times right but uh so could could it be biased to words smaller words in terms of when we do uh this kind of yes it is okay so even if
(1:03:15) the word is long a lenier word is repeated multiple times it might be lost because of this thing yes okay thanks p hi uh I have a two-part question firstly I want to check um uh when we do any iterations right if there is a same number of tokens or like bytes how do we break the deadlock for example in the example you said is first one first one first out okay and uh uh okay and is is there um any kind of research areas in tokenization or did just this BP is the winner no the there's a lot of research and people are
(1:04:03) not sharing what they're finding it I see open I being very silent about their new tokenization process we we have access to the last uh tokenization used in Gemini 1.5 and gbd4 after that people have been sort of mute and you can understand why H okay okay thanks so uh for example if you want to uh build a tokenizer for the indic language or for example let's say Gujarati or Hindi how would you like how would you approach this problem yeah so that is an assignment literally oh so thank you for stealing
(1:04:46) the Thunder of me explaining the assignment to everyone we going to we going to look at it don't worry today only Sha uh when the tokenization process starts so initially they would have like zero vocab length so do they start with a fixed text and then find the probability of each yes we start with 256 first all characters of asai that's the next topic I'm going to discuss and then they start finding so first 256 are always going to be same for nearly all the t tooners uh not the gap size but the text uh where
(1:05:19) we where they find the probability that's depending on the data that you're training on so it can be the internet data open source data all the search papers and so on so people will have to look at that and that and that's why the data set is also very important okay and then the pro then they don't add any new thing no okay then they don't add any new thing no so you you you uh do your statistical analysis on the full data set okay okay got it John yeah Ran So I see that BP is more language agnostic compared to previous
(1:06:04) uh models but however there are some challenges in terms of complexity and the processing time right I mean you to run more iterations and also the token size would also increase which means the cost will also be bigger right no token size we keep fixed so we say make a 100 one make a 200 K1 make a 50k 1 this 50k so we start from that that give me 5 50257 tokens okay which which also includes all languages which also includes all all languages are already included I I'll explain to you how okay okay but whether we keep them at the Character
(1:06:39) level as we are seeing in this case for Hindi we are sort of literally on a character level right anuad and da and so on or we are able to convert that into a word that really depends on how many tokens we want and how big data set have we picked okay okay and so uh in the previous example that you're showing a BB can you show that uh Rohan I need to write it again so I'm going to do that in okay two minutes it's okay it's okay Ran So in that example when when does the iteration stop when you when you have reached your
(1:07:20) total Target oh okay we specify let's say huh you you say I want 12 you say I want 1400 you say I want 3500 35,000 and then the algorithm sols oh okay okay understood yeah thank you s yeah so I just asking this is tokenization is part of this first step uh in the Transformer models okay so now I'm asking like is this part is this part of a model or is it just a program which creates the token like it's outside the transform model it's a pre-processing step even before we send data to Transformers so so this is not any kind
(1:08:05) of model this is just a program is go through the data and take out the tokens correct okay PES uh yeah so is it like some neural network is used to get simple algorithm we read that algorithm literally that algorithm what I did on the screen this is the algorithm okay one more doubt is uh like suppose I have two words in my vocabulary which are in the same Suppose there is some word called Manan and there is man and man and moan which are already in my vocabulary so does it do based on how frequent like which is
(1:08:47) frequently coming based on correct corre yeah so my Mo I don't think will be converted into one token ever because of man and moan coming separately you can see that in both the cases man M yeah so man M has in fact occurred more compared to one more so even moan is part of my vocabulary uh because man is more frequent kind of it splits according to the even Manan is not a part of okay yeah even in case of that okay got it I understood thank you okay PES Rohan previously if you remove that Manan right 127 127 were shown in the
(1:09:42) same color in both the text but 84 and 84 were in different colors even though it is taken as a token color don't go on color we need to look at the uh token number number below because as I keep adding it changes keep changing the color oh okay so 8 I don't know how to find 804 yeah 804 is 39526 and I also see one more 39526 on top that I be color and token it does have a correlation no no no correlation you can see that there also we have 39526 3956 3956 okay fine yeah thanks okayish so uh ret tokenization would
(1:10:32) mean uh complete training from scratch or is there some incremental uh kind of process like G training from scratch you change the tokenizer you have to change the r model so GPT as they are adding more kind of different languages they have to train from scratch they have to train from scratch oh okay because the position of the tokens will change in the finding order okay so if someone can find an incremental thing that will be a begin I mean yes yes mil uh so so how this model performs if I am sending for example uh if I'm
(1:11:16) sending text to my friend in Hindi but rather than using Hindi I'm using English like uh uh English to you know send other non-english languages then uh if my tokenization will be po right in that case if unless and until I give it depends on it depends on how much data was there for Hindi return return in English Okay so it's basically on how what kind of data you fit while tokenization yes and also uh the maximum length of the token so so the token size if you see the words represented by each toen are kind of small in size in this
(1:11:52) example like four five characters not more than that uh are there cases where tokenization itself I mean by poer coding itself gives a very big to yes we will look at it yes okay do do we uh apply some sort of limitations on how much big tokens SI should be or no okay and wish so uh how come uh when for the latest models that you're showing the token count got reduced is said because of increase in vocabulary or looking at 200k here compared to 50,000 here gb2 is ,000 this is 100,000 200,000 okay okay okay iterations they
(1:12:39) have increased iterations and increased the token count yes and now they have to pred these 200k tokens also every time so it's a heavier model but 50k is a really good number you can just see that for English okay got it thank you yeah so uh if you look at that Tik toer output for Hindi uh so this so here it says if you look at I lost you word d and there is also a word yeah so what I was saying is there is a word d huh which basically means from and then there is a word d which basically means door so here I can see
(1:13:21) that it is a single so it is a single token whereas it would have been better if there are two tokens which is DW and one token which is I so you can have more combinations of word just from two tokens see you are talking like a linguist expert when we when GPD is taking Hindi data they're not looking at Hindi experts and asking them for these rules these rules are there for English we're going to see that that's why I'm saying when someone says that they're training for inding languages and
(1:13:46) they're not discuss what you're just discussing right now they're actually not doing Justice correct correct so so basically for for this to this tokenizer to work perfectly it basically if we feed more data that's what my point was I was coming to that that if I feed more data maybe it will understand that I need tol no you need to come up with your rules and we going to see that in a moment okay okay give me five minutes we just going to see that okay one last question is there a benchmark for IND llm no is
(1:14:23) there I'm not not not I'm not aware of should be not here on how this BP handles unnown words or characters but now everything can be done because now all characters already included so if we this let's let's make any unknown word we can think of say D no so this this is Latin so I understand Latin must be uh exhaustively covered but some character which is not Latin like something coming from Chinese or Japanese the original from Andre was Korean so I you can copy paste that I copy pasted Hindi you can see everything
(1:15:12) is covered if you can write it on internet everything is covered why we're going to look at it in a moment okay okay last question yeah Ran So I I think it was already asked few times uh followup question on that so because a lot of text which is available on internet even for Hindi it is in form of Roman form right so uh with that data set even if it learns Hindi it might be it might do very well for this form of data right the home Roman English rather than because there there would be different bites when we write it in that
(1:15:51) when you wrote is Google that is a different bite whereas um if you write it in Roman it is a different bite so you can see that this hair is also covered properly and I'm hoping that this is coming from the Hindi part or other stuff mes for example is covered so it really depends on the data that you have and what data you have train on so when you said that there are no like people are not training on Hindi data you meant for that is what you got right no and this also right so do you think gpd2 would actually go through uh
(1:16:25) for Indian forums and collect data from there to when Indians write in English in Reddit this data is going to be there properly on WhatsApp so maybe Facebook has trained okay got it but this is still an easier problem to solve right most of the data which would be in even for Hindi on Reddit you can collect but the N data that is very rare at least on internet not published enough so people so Google has now scanned every book and then you're taking it from books and other stuff okay got all right let's okay thanks so
(1:16:59) now let's go back to this thing and uh we saw some of this stuff now we also saw the web app and we work with tooner now what is text or string and how is it represented so this question goes to Python and python says textual data in Python is handled by the string object strings are IM sequences of uni code points this is what we're interested in what is string strings is a immutable sequence of Unicode points what is Unicode point now strings are immutable sequences of Unicode so let's go back to
(1:17:32) Unicode what is unic code uni code sounds like one code it is a uni code standard so it's a standard and it has defined 15499 characters for 168 scripts and when you look at the scripts we're talking about uh different languages here 168 languages ma major languages are covered in these 116,000 stuff and you're talking about a lot of different languages including all of this which we can look at right we have Hindi we have Korean marati and so on and so on and all of those characters combined together just 15
(1:18:02) 15499 characters and that includes 3,790 emojis also and you might have heard of this utf8 UTF 16 UTF 32 especially when you get errors right we learn more about things when we get errors and utf8 is something that we interested in and uni codes like utf8 let's go back to what is utf8 that's what we are going to be using now utf8 has 1.
(1:18:26) 11 million uni codes all of them represented in eight bites okay we learned that we have around 7.5 million words here we are saying we have 1.11 million uni codes we can't use all of these uni codes so we have to come up with a strategy that where we have something somewhere in between but we're going to see how and this one one and so on so on is going to represent all the text that we have in some sort of bytes now what happens is uh and you will also agree with something like this now let's say I have a digit 42 if I can represent it in one
(1:19:00) bite but my data type is of four bytes how many bytes you're actually going to use to represent 42 one only right for compression but if I want to write like 4, 44,4 44k and 444 and so on and so on and needs four bytes and I'm going to be using four bytes so what we mean by this is if I have a list of things right and this list of things is 1 2 3 4 Kaka a b CD and so on and so on so initially I just need one bite to store this because this information can be stored that number basically 1 2 3 four number both
(1:19:30) can be stored in one bite M across a threshold of 2 157 then I need two bytes to store them properly and then so on and so on then I'll cross threshold again then I'll need to store them in three bytes then finally I have to store them in four bytes is this clear why we have to increase yes alternatively I could have wasted Space by storing everything in four where where all the stuff initially is zero we're not doing that this is what this table is basically saying so for some of the things we'll have uh few
(1:19:55) bites and for some things we have large bites so the first few asy 128 code points need one bite the next 1920 needs two bytes and so on and so on and in the next 1920 quote points covers all Latin scripts alphabets IP extensions for Greek cic Coptic arenium Hebrew Arabic syak tana and a alphabet as well as combining diacritical marks also I don't know what it means but you understand that by the time we're hitting this 60 1,44 40 code points we are looking at multi language plane that includes not Chinese and Korean characters also and
(1:20:31) by the time you hit 1 million and so on we cover everything including emoji and other stuff that's there so this is going to be a starting point we're going to start from utf8 and we're going to use that to make sure that we can actually cover every single language now now is it clear everything that has ever been written on internet is a part of UTF already and we're going to use start from UTF to make sure that all of those are covered and then come to some 50,000 tokens that we want from that or maybe
(1:20:54) uh something that is there in uh let's say German or Spanish or Chinese or other languages are we good any question okay now let's go back to BP algorithm again now this was explained on Wikipedia so we are going to look at this example again and someone also wanted to discuss this so we have here and if we repeat it we have going to get a a a a again then a b then B and D then d and a then a and a then a d a squ then a a squ a again and then a b my mic is not working can you hear me I just lost my mic for some time
(1:21:56) yes okay again a a a a a b b d d a a a a a a b a and a see at this agage we have to read ABCD anyway so now we have a and b and c and d already taking by space of one two bite number of 1 2 3 and four and then we going to see that we have a a and a a and a and a repeating most number of times so we are going to to call it X and we five byte then we going to rewrite this as x a b d x A B A and C cool and then you can see that we have y coming in now can give it C and this was AA and this is going to be a and b just
(1:22:45) looking at this then we can write this as x y d x y a and c and then we can use Z and give it a seven space and now we can combine X and Y right so this become Zed D Zed A and C clear so 1 2 3 4 5 6 7 8 9 10 11 11 tokens if you were to write at character level have become five but our four vocabulary has not increased to seven are we clear and now you can see how Manan or other words could be made because we had X and A and then we X and Y combined together so right so we combining these things together and that's how we find these repetitive use
(1:23:38) cases Jay I don't so why did you use only two characters say we we can see Triple A is repeated two times so so it needs to be the in the part two if the Triple A is twice then it needs to come in the part two if that was saved okay so there is a bite pair we focusing on two pairs together and then eventually if the data allows it becomes 3 four five okay okay so now when we say that we have 50257 vocabulary size in gpt2 this had 256 initial uh tokens that has taken from the utf8 and after that we ran this
(1:24:18) algorithm 50,000 times to add 50,000 more so 256 plus 50,000 becomes 250 6 50256 and the one last token that's there that is there for the end of the line token basically end of the text so we tell our LMS that the input to you is done or LMS also give this as output to tell us that okay I've given you all that I could p uh yeah for every iteration I seen that the input string gets changed uh with the new uh BP added correct correct correct so is it a is it there any Merit to uh have the original string still place
(1:24:58) because uh things like able a and double b are still there in the original space let's say but they are now changed because of what do you mean keeping the original we can't delete the data set if that's no no what I'm saying here is uh see that Z is equal to AA right but it has now lost uh things like ba being there again let's say yeah so what I'm trying to say is should we be keeping da again as such alongside Z so for decoding we need that okay okay so let's head back to our code
(1:25:41) now okay so now we are going to look at more of python here is this function written in uh python where given a character it's going to give me back the ASI value of that right and we have asai value it's the same of a that we looked at for lot of stuff for emojis and for R also and you can see ra is in 2352 so not the initial 2100 we two bytes for you so ra is going to be in three bytes so not only we have English and other things using less bytes to represent we have other languages that need more
(1:26:12) bytes to represent right so the compression also is not there because we have not prioritized them compared to other languages is this clear so I can use this function now to actually get the of eight characters for all the Hindi characters that we have is Google translate so now if I do this I'm going to get all of that text which I have no idea what this is but you can see that we are going up to 28,000 and that properly is going to be for the Emoji that is here 1 28 5 and 7 1 285 8 is for eight bytes right sorry
(1:26:48) UTF 8 is 8 bytes right utf8 utf8 eight is 8 bytes 8 bit bit then how that like more than 256 will be accommodated here you are using more bytes to represent okay so by one by two by three by four UT yeah okay okay so each each bite is a ut8 that's what you're telling ut8 is a convention name of a convention where we have this first 256 and after that we use two byes to represent more then we use three byes to repres more then we use four byes represent more and we can keep on increas increasing it but the inherent
(1:27:36) storage of anything is into B bits only 8 Bits okay okay so now if you go back to our code we are going till here so you can see that that's around 150,000 characters and the unic codes the problem with unic code basically is uni codes may change in future there are a lot of unic codes we have 150,000 uh them and we wouldn't get text compression for example 13 words that we saw on top here if we were to directly use uni code only for our LMS this 13 word basically became somewhere around 57 57 unicodes so we can't use unicodes
(1:28:12) also clear and if you just want to look at the uh bytes then you can just look at it like this so these are the bytes if you can read the X values are we good and uh if you want to look at them in the utf8 format as we did earlier then we can encode them in utf8 and you're going to get the same number back so here we have two you need to consider two and three together basically to read one single stuff I hope you get what I mean if you understand what this is then look at this we have e and we discussed that this going to be in the third uh 3
(1:29:00) by system not in the 2 by system so I should get three numbers for this and you can see we got three numbers for E clear here by one by two and by three good yes no all right now let's start some more programming and then I'm going to share a bit of this video for you to understand some of the other stuff but now we can do write a lot of code okay so now we have k equals to Lambda for S given and return me s^ two now I can do a list of a mapping function that is going to map this function to all the numbers
(1:29:53) that are in this list 1 two and three is this code clear okay now 1 2 three I can remove uh change by range of 10 cool now yeah Ron sorry uh was just thinking through your line number 21 so if it is taking three bytes yeah to represent a single character in Hindi then with the bite pair encoding which takes two bytes at a time will will this work for indic or we have to use three bytes at a time for I'm going I'm writing bite code and coding in front of you so you'll see okay okay now uh we're going to go to this
(1:30:52) blog because this block has a lot of characters and other stuff also and we're going to extract text from that so and copy paste some text from it what we're going to do is basically this so we taking text from this website we copied some text because it has unique code some symbols and the stuff you're going to encode that into utf8 to get the raw byes we are going to take the tokens and we're going to take integer and we're going to convert that integer and map it and we're going to get the
(1:31:16) list back and we're going to print line text length of text and tokens basically so here we're going to see that that we have all these this text converted into these tokens and some of them because it's all English I'm hoping that we have one for that but for uni code maybe we have a lot more and total length is 66 is this clear okay okay now I have a list of L where I have 1 2 3 4 and five now if I write this and I'm going to get the list back but if I write this I'm going to start from the
(1:32:02) second number is this clear yes okay now I'm going to zip two things together I'm going to zip my list and I'm going to zip my list starting from one ending at this and then I'm going to put everything in a list so what are we doing so I had a a a a b b a c a b a d e something and I want to do that combining thing what I'm going to do I'm going to start one more but from this side a a b b a c a b d and e right and I'm going to combine them immediately make my pairs are we clear e will not be look that that's
(1:32:50) what we're doing in this ZIP thing so if I now run this I'm going to get a pair of what wrong did I do L I was just writing one L is this clear okay now let's talk about dictionaries a bit so we have a dictionary and uh do you know in in a dictionary I can write one and four yes or no I can write a and red you know this do you know I can do one and two and five why why will this work dictionary needs this thing to be immutable first and hashable right and a tle cannnot be changed tle is immutable and it's
(1:33:54) hatchable also have we good on this okay so if you're good on this then I'm going to just write one simple dictionary in this dictionary we have a tle of one two that has a value of 10 then we have a and b that has a value of 23 and I have three that is a value of double of 45 and 56 clear then I can say D do get 1 from 2 and I will get some value I'm going to get 10 there clear if I write three I'm going to get nothing but we don't like nothing so let's say if we don't get something give
(1:34:39) me na are we good so I can write zero here so if I don't find that thing give me zero similarly I can say D do get and one for me and if you don't find anything give me zero are we good here okay if you're good here then write let's write this function define get stats function I'm going to be sending in IDs and count is going to be an empty dictionary and then I'm going to say for pair IN Zip of IDs that I'm going to get and ID is that I'm going to start from one so this makes our the pair thing
(1:35:28) that we're talking about then I'm going to get counts of pair sending the uh tupple sending the tle inside the dictionary like this and I'm going to say is equals to counts basically go back to the dictionary and find me this pair right find one and two and if you find it perfect but if you don't find it then give me zero are you clear and if you do find it then just add to add one to it so basically we are saying that I have disle 1 one 1 2 1 1 and two and two for example just give you an example
(1:36:12) okay and uh we have ran this one already so in my dictionary I already have a pair of one and one and that is stored as one because of this plus one so this is gone next time I run this I'm going to check for this SP check for this pair in the dictionary and uh I need when I'm saying uh my dictionary of three dictionary of 3 is equal to 4 I can assign it a value so I'm assigning it a value and what is the value counts basically go back to dictionary get the value so for this I have 10 already and
(1:36:43) add one to it if you find it so I'm looking at it for the second time so this becomes two if I don't find it then basically this is not there and uh I don't find it so I get just zero this thing 0 + 1 so we initialize it with one clear on this code and once you we going to return our count the dictionary and then we have stats is equals to get stats on token that we saw above that we wrote above and finally we are going to print our stats dictionary and we also going to do print of sorted of value in key for key and value I want to
(1:37:29) look at the M RSE order in st. items and I'm going to be reversing them as well rajendra uh yeah can you explain the dictionary initial again uh basic code if I I'll recommend that you go online and do more exercise on that what exactly you want me to do here on the dictionary side no what I mean is a dictionary comes with a key value pay right so one of two callon 10 I did not get it what is value here and what is key I mean what we pass to get the value actually you're talking about this line no the the line number
(1:38:10) 34 where you where you written D equal to 1 comma 2 coln of 10 yeah okay yeah can you explain it again I mean how the initialization Works actually I have no idea what you're asking about but let me try D needs a key and a value all the keys must be hashable which means I can use 1 2 3 4 I can use a string called red or a character red also I can also use a tle because tle is also hashable so that's why we use a tle here so you mean one comma 2 is a key right that is my point this is the key okay and then 10 is a value 10 is the
(1:38:49) value okay now on the second part you have written d. get one of three so one of three do you have any sort of Val any sort of key matching to one of three I do not have if I don't find anything please return zero that's zero is got it four of one if you don't find it please return zero yeah okay so this is the uh so I've ran it on the tokens that we found here for this particular text and this is what we're getting these are the key and values that we are repeating actually and if I scroll down you're going to see that
(1:39:22) some of the things here fix example uh I'm printing both at the same time okay yes so 10 and 32 was repeated 20 times 240 and 159 was repeated 15 times 226 and 128 was repeated 12 times are we clear okay now if that is clear then we're going to say top pair is equals to maximum of stats and key is going to be our stats.
(1:40:07) getet and let's look at what is the top pair same thing that we found ear one1 and 32 and if you go and use CH on CHR on both of them you're going to see that it's talking about e and space so it's finding e and space a lot of time now if you go back to our text you going to see actually that you find e and space a lot many times e and space then e and space here then we havean space here then we have and space here you can see that right and if you were to count this you're going to see that this is 20 number5 so that's what we have found in
(1:40:47) our programming as well all right so we got that now we are just going to convert that into a big function same thing I've explained but now we've added that to function so not going to explain this function just uh sit and try and understand this but now we have a list we're making a merge this function is called merge and we looking at 5 66 7 91 we're trying to find 6 and 7 and if you find 6 and 7 we are passing it a value of 99 we clear we are sending IDs we are sending a pair we are sending the ID X the new ID to
(1:41:32) represent that pair withd 99 so new IDs are stored here for I equal to z i is less than equal to the length of the ID so I'm going to go through all of them so I'm going to go through 5 6 6 7 9 and 1 and if I is less than length of IDs minus one then stop and if ID uh I is equal to pair of zero so pair of Zer is going to be with this and ID + one is going to be the pair of this so basically I'm searching this and this and this and this and this and this and this and this and this and this right so
(1:42:02) if I find 6 and 7 which is going to be this condition then new id. append new id. append idx whatever ID we are sending which is 99 that is going to be appendant appendant in this list you clear yeah okay so if you look at this code it's basically this is how that b pair uh Eng there finally it's going to work right so we have just replaced 67 by 99 here that's his score now we're going to do two on that thing again but now we're going to send all of our token we're going to send a top pair that we
(1:42:38) found above which was one1 and something we're going to replace it by 256 so now if I run this you're going to see that we started with 616 now I have 20 less tokens correct because those 20 are now repl by 20 into 2 replac by 20 256 101 and 32 is replaced by 256 now 120 101 and 32 were found how many times 20 times are we good those are replaced by 256 so we we are removing two and replacing it by one so then hence a reduction of 20 here okay now we're going to look at a slightly longer text and that text is
(1:43:31) going to be this one we're again taken from that website only but it's much longer text compared to what we were looking at earlier so I'm just going to uh take all the text that we have we're going to encode that into utf8 then we're going to map it with the integer so we have our tokens are that are mapped into an integer list a tole list clear then we're going to write the same function that we did earlier making a empty dictionary looking at a pair and adding the number of count we found that in
(1:44:08) that particular text then the same merge function that we wrote above so these are two functions that we have seen good yes or no okay if you're good then now we're going to write something we going to say that our vocabulary size is going to be 256 276 we just adding 20 more are we good okay that means our merges are going to be 20 additional ones 256 minus uh 276 - 256 so basically we are asking it to run 20 times okay then the initial IDs we have is in the list of tokens we are just soring it clear now we're going to create a mergers
(1:45:00) dictionary integer integer and for looking and storing it as a integer value we're going to run it 20 times we're going to get the stats we're going to pair one we're going to increase the ID and it's going to tell us merging pair into the new token idx and ID is going to be merge ID pair and idx the function that we wrote earlier and it's going to be storing that pair with that ID now if you run this you're going to see 20 outputs 1 and 1 and 32 were given 256 105 and 1 1 Z was given 257 and so on
(1:45:29) and so on and finally we stop at after 20 iterations so we are right now making a 276 tokenizer earlier for gpt2 we saw 50,000 so if you run this 50,000 times then we going to be doing what gpt2 did clear okay now here comes an interesting thing here we're looking at the original number of tokens okay then looking at the ID that we have and we're looking at length of tokens and length of the IDS now this gives us the compression ratio so total length initially we started with was 24,000 just in 20 iterations we
(1:46:06) have dropped it down to 19438 so this number divide by this number gives us 1.27 are we good okay so going back to your assignment in your assignment uh there's a mention that you have to hit a compression ratio of three 3.2 or above but you cannot have more than 50 5,000 tokens right so that's I hope you get some idea from looking at this code now I'm going to run the same code again but this time I'm going to increase my vocabulary size to th000 so 1,000 - 256 is going to be the number of times I'm
(1:46:40) going to be running this algorithm and I'm going to run it on all of them and let's see how many times this basically does the compression for me I get a 3X compression are we clear all right so here here uh I just have some text for you so tokenizer is a completely separate independent module from LM it's not part of that tokenizer tokenizer is something that converts your raw text in a uni code Point sequence into something that can be converted into sequence and that is sent to llm right and we are using bite
(1:47:26) pair encoding right now and transs back and forth between the raw text and sequence of llms right it's not part of LM it's something that is separate from that are we clear okay now this is a code that is going to take those bites and convert it back into the token for us so if you look at if you send one number it's going to come back and tell us what exactly it is representing and if you look at a merges dictionary we going to see that we have from 256 to 999 the tokens that we converted into and
(1:48:05) compressed so we can write the reverse function where we are encoding it so ear we saw a decode now weing encode so here we are encoding an empty string so we are not going to get anything but if you say red then we are going to get some token for that that is going to be 305 and 110 are we good and if you write hello world you're going to get these tokens for that the best way to test and validate our tokenizer is to take the text encode it and then decode it and then show it as text two and text two must be equal
(1:48:42) to text one and is there's a equal to equal to Quality now if that comes back as true that means our algorithm is working perfectly well all right this is working well but this is the most riment uh bite per encoding that we could have written that is not what has done what is done in the GPT paper so now let's go to our GPT paper which is this one and here we're going to talk about I'm hoping this is s sounds like gp2 only our largest model gp2 yeah so let's GP okay so B encoding is a practical
(1:49:20) Middle Ground between character level and Word level language model which effectively interpolates uh by the way if somebody one of you is thinking that we saw e and that had three bytes right so we will representing it as three bytes there so first this will be encoded and then this will be encoded just to encode e are we clear right let's say we have a I'm writing in English we have a car and how do we WR this this only so this would have three bytes three bytes three bytes three bytes these will be same right so they
(1:50:01) probably would have been converted into a single number and that number and this number will represent car that number and this number will represent car right so we are using bite per encoding but we're not using those 1.1 million of them we're just looking at these numbers and we going to find a lot of repeating numbers here and that's how we're collapsing this is 50k or 100K or 200k okay coming back here uh this is name reference VP implementation often separate the unic code often operate on
(1:50:27) unic code points and not bite sequences the implementation would require including full space of unic code symbols that is around 13,000 before any multi symbol token can be looked at this is probility large compared to 3,000 or 64,000 token aies that the BP uses in contrast BP level version BP sorry B level version BP requires a base V cap of 256 however directly applying BP to bite sequence results in a suboptimal merges uh due to BB using a greedy frequency based on htic approach we observe that egg egg dot egg exclamation and X question mark
(1:51:01) are going to be very very differently handled by these token these ra bpe this results in suboptimal allocation of limited vocabulary slots and model capacity to avoid this we prevent BP from merging across character categories for any bite sequence we add exception for spaces and this stuff this is what I want to focus on this is where we H we have to override some of the things that we do with BP and to understand that we're going to go to our encode file which is going to be inside the gp2 gpt2 paper so we we seeing an encoder here do
(1:51:36) you see that encoder py I think last you are you seeing the encoder at py yes 100% and someone wanted a break so here a break for you okay let's take a two minute break theant
(1:52:45) nanali okay S I have a you said that okay you go yeah so I wanted to ask why did you choose to run the white PA and coding algorithm only 20 times how was it predetermined uh why did I choose to run it 20 times because that's the number that came to my mind if you want I can run it 21 times okay so like when we run it for on the actual data set we have to let uh one second so when we write these llms we need to decide how many tokens that we have and what is the output that we have right now let's say we have 1
(1:53:23) million tokens which means the output has to predict one of these 1 million tokens probability large number and probability large number to input also so we keep it something like 50,000 50,000 basically means that I'm going to run my algorithm 50,000 times so this is actually 50,000 for gpt2 gp4 said I can afford up to 100K then GPT 40 said I can afford up to 200k of course I need a much bigger GPU than that so this algorith will be run 200,000 times or 100,000 times or 50,000 times depending on how vior tokens are
(1:53:55) so this plus 256 plus end Define token is basically a total number of tokens that we have so gpd2 has 50257 tokens similarly when we say 100K we have actually 1 247 or 200 we have 20 2557 tokens got it h parameter okay thank you okay M here what was the compression uh how did you get the compression number uh the new old tokens versus minus divide by new tokens so what is where you started before merging okay a a a b a a b a so 1 2 3 4 5 6 7 8 divided by x a b x b a 1 2 3 4 5 6 six got it so uh we'll do it uh from the
(1:54:53) first and to the last right like after 21st that's what you're doing like for example you ran it for 21 times yes okay thanks okay NES yes yes R1 sorry uh so I just had a quick one uh you said here that you know the tokenizer are completely separate and independent module from the llm yeah so I just want to understand uh so these tokiz ation Works only uh during the training of uh the models not during the real time when we are using it like GP need you need a decoder at that time you need an encoder and
(1:55:33) decoder both at that that time because you send in English I I can only send English now what I I mean it I I want to ask like during the uh uh you know when user is actually using it or the validation period does that tokenization uh changes or uh improve or add on to the existing it is fixed fixed okay so once it is fixed and the new model is released like lately gp3 is released then the tokenization uh doesn't change I'm not sure but they may have made a new tokenizer based on their understanding okay but once the model is
(1:56:13) released after that it is not getting changed for that at least that model that model correct okay okay SATA we're talking about this 50,000 tokens or 1 million tokens so what is the problem uh if like where we are limited here so uh it is a one time run like one time means like that one one second one second first of all uh llm model has three parts first is the embedding layer then we have blocks repeated 12 times or 18 times or depending on which model we're looking at and then we have the output layer
(1:56:51) okay okay now let us say say I have 1 million tokens to be predicted how many outputs I have to predict 1 million yeah yes right and similarly here embedding I have to create for 1 million tokens correct now when I send I am Rohan and or I am Steve for example I'm expecting it to fill in the blank I'm going to be sending one two and three picked from these three 1 2 and three that is sent in correct and it it will have 768 dimension for itself also 768 each Tok some 768 or 1024 Dimensions right so uh 3 cross 768 dimensions are the input
(1:57:35) taken from a search from 1 million tokens okay got then I have to predict 1 million numbers so the fully connected layer before that will be at least 768 these dimensions cross 1 million yes that means I have 768 million parameters in this layer and I have 76 million parameters here the size becomes big when you taking the we can't go with that but GPD can can NES I done hi um um so I understand this U bpe is about token addition right simply what is the bite pair part I missed it what is the bite P part we missed
(1:58:34) after all of this really kidding me see I still see this is a token addition that's about it token addition of the bytes of the unicodes these are the bytes of the unic code we converting that into this is a bite pair converted into a new bite okay but I can still visualize them as a characters right these are characters index these are character index no uh for the 256 yes but after that for each character I have to represented in two bytes right so two bytes are representing one character you saw e right e required three three bites so we
(1:59:15) are finding these pairs and replacing it by one new bite that is why we call it bite pair encoding you find the bytes that are repeat it twice and we incode that into a new b h i I didn't understand this part out of the three why are you doing two bites because it's a bite pair pair of two people marriages between two people because of the algorithm nothing else I see and uh if you think about this also I also wrote G uh G and so on they will be represented by 3 divides yes no yeah but these are going to be
(2:00:00) same only some of the msbs of this is going to be changing right lsbs sorry yeah yeah given that the frequency is similar not not frequency let's say I've exhausted 260 numbers before this this is going to be 2 1 uh 61 21 62 21 63 and so on 2 1 6 4 just by definition these initial two bytes are going to be same for all of them right so I can represent all of them by a new bite let's say called X and then let focus on this one so x61 is car now x62 is K now mhm and then I can combine this if I'm
(2:00:43) finding K A lot of times okay thanks that's for white pair okay now we will looking at our gp22 paper so we are looking at the encoder from there and I want to look at byes unic code we wrote that we looked at odd and other stuff so same code get pairs exactly the code that we also wrote that's also there but now you're seeing this thing this is a part of something called Rejects and these are the laws I'm talking about and I want you to understand what this is and uh better than me I'll get this tool
(2:01:18) called Char to explain this to us so when you see something like this you're going to see that here we have saying rejects basically what it's going to do whatever text we're going to give it to it it's going to look at apostrophe s apostrophe T and apostrophe re apostrophe IV apostrophe IM IM and separate the word there right it's going to match the most common English contractions as apostrophy s and t and re and IV and M LL and d and split it there then it's going to match the list
(2:01:49) of numbers so if I say for example hello 1 2 and 3 is going to find a sequence of numbers and it's going to separate it there this is numbers and similarly it's going to find a sequence of words going to separate there this is being done before the bite pair encoding also works what are we trying to do let's say I'm going to send I am uh friends cars Windows button okay now in this case if we were to run bpe the way we do then I'm going to be combining these together yes no instead of combining these
(2:02:46) together so instead of sending this if I can send I then space posty M then space then a then space then print then space then the posty S then space then car then apostrophes space then window space apostrophes space button and so on now in this way if I send it then it's going to find apostrophe S A lot of times and it's actually going to incode that for me yes or no yes so that's the that is what this reject is doing and in fact here itself you can see that they're adding a command should have added the re ignore
(2:03:23) case so BP merges can happen for capitalized version of contractions also so we're going to see what this means basically but let's go back to our Rejects and we're going to look at a very simple Rejects and reject is going to be called from library called rejects so we have we have a pattern here re.
(2:03:36) compile here we are seeing that if you find a continuous length of uh characters just separate and give it to me so now if you run this you're going to see that it's going to give me tiger then something then something then s s separately so we are looking apostrophe in a different way then de and then I I is there then V is there then killed is there and it is there and so on clear we can add one more rule we can add a new rule where we are saying that apostrophe s and apostrophe V please count them and look at the length of the
(2:04:09) continuous length of the word and continuous length of the number and then give that to me now if you look at it slightly better it's going to give me tiger and aposh S and dead and space and I apost V together and killed it now if I go here killed 1 2 3 and now on it you're going to see that 1 2 3 is also going to be given separately to me are we clear so this is a pre-processing that we are doing even before we send our text to bpe any questions on this prsh no no it was something else uh so BP seems to be a very fixed
(2:04:45) algorithm and then I see that this kind of heuristics I mean some human looked at it and wrote this compile so why can't uh some smart algorithm can be written to figure out like this kind of reg believe I believe the answer is there but Google and open have not made it public yet okay okay okay so now what we're going to do we're going to look at a GPT series um Rejects and this thing is basically copied from here this Rejects cor exactly same let's use this only okay so now we are sending hello hello world 123 how are you let's
(2:05:41) run this and you're going to see that it has split in different manner that we are interested in are we good similarly now let's send Hindi the and you're going to see that it fails there it is going to combine all keep all the characters as much as separate as possible it's not able to figure out that anad is actually one single word where here it can understand the world is single word now do you understand my pain when someone says that we have trained on inding languages you are you have not even written a good reject
(2:06:17) there to handle something like this and then you're saying you written a uh that's where we are basically stuck so you have to come up with a really good reject strategy for Hindi words and other stuff especially for the Indian languages that I'm looking at now do you read that comment here the comment said that we should have added some ignore case for rejects what that mean is that now if you run this where we have house on top where we had house it was able to do how and apostrophe s separate out
(2:06:46) properly but now if you run this you're going to see that here it's going to fail here we have how apostrophy handled separately and as separately are we clear yes or no okay now let's look at this part where I've tried and added a lot of space here and after you run this you're going to see that it actually identifies that lot of space together so the actual Rex written was able to handle the python code but later on they all collapsed into single uh one for gpt2 and not able to figure out but now
(2:07:20) let's run this on python code right and now if you run this you're going to see that we have a lot of spaces managed properly but again this was not handled while training so we lost it there clear okay now we're going to install tick token that same library that we looking at so TI toon is installed and now instead of copying encoding all of that we just going to say the Tik token please get the encoding for gp22 for me and encode hello world for me and now we saying tick token get the CLK 100K base
(2:08:01) for me and now encode hello world exclamation exclamation for me same thing that we were doing at the ti tooner here going on top and changing all of this now we doing it via code now here you can see the hello world is converted to 220 220 220 that is this space and then 27 through 37 and so on and so on right and here you can see slightly different so both the encodings are both the encodings for BP are different clear okay now going back to uh this gpt2 rejects that we had they were able to handle Ive properly I and post s was handled
(2:08:44) properly and now this one is GPT 4 reject so slightly different from that and now if IUN run this even the Capital One will be handled properly so IV for capital is also handled properly are you clear if you don't understand what is this just ask charb it can break it down for you we're not looking at a as a code as a exercise for okay now let's run these two things we are downloading the vocabulary for gpt2 and we downloading the encoder for gpt2 and now should be stored here and I'm going to look at incoder of
(2:09:32) Json and here you can see that if I can download this okay so you can see that zero goes to exclamation mark then we have backs slash and we have these signs and uh uh a starts at 32 as expected uh and until the first 256 we have all the ceric and other characters taken care of and this is where the bite coding coding stops now this under this g dot is actually the trailing space that's how it's represented here so now you can ask for searching for a particular word so let's say we search for Mercedes that's actually covered as
(2:10:26) a word uh now let's search for but look at where it's covered 21, 272 whereas let's search for uh Stanley Stanley is 14,000 before Mercedes now let's search for contr F 20 and that's 29,000 very great uh now let's search for Washington 2669 you see that the data now you can clearly see how often Washington was represented in that data that the whole word itself is getting one full description not I Del it's going to be way and if you write Bangalore not there 46,000 just made it contr f
(2:11:07) t 49,000 just made it look if you look at final ones you're seeing that this is where it stopped basically right and I'm not sure Hindi words are going to be there for example mirror I need to copy this not there right and capital M also might not be there yeah so here you get an idea of where that particular word is after 50,000 iterations of this algorithm so you can search some words and try and understand how much of the data I looked at here so do words like Ambassador that have gone out of existence also will like these are
(2:11:45) products that have gone out of existence do they also exist here you will have to search for them Ambassador not not not there it can right no data if you have the data for example Egypt is already gone right or far p h a r o spelling spelling p a far is also not there I'm not sure what the spelling is or Spinx s Spinx is also not there so depends on the data what data you have trained it on you train it on egyptology then those words are going to be here right so this gives you a really good idea of what these uh things are and
(2:12:26) what are all of these characters so let me quick this H myself again and now let's run a bit of code and that code is going to look like this so import OS and Json open encoder Json R encoder is going to be json. load open vocabulary BP BP data is going to be f. read now BP merges is going to be tle of merge string split for merge in the BP data spit what is it doing basically it's very similar to the merges code that we wrote earlier so now if we look at the encoder encoder will have a l length of 20
(2:13:05) 50,2 57 if it prints why yeah right as expected and if you look at the type of encoder it's going to be dictionary lit a search and if you look at the last one which we know must must be the end of line End of Line you're going to see the same here also that is also the end of line 50256 here okay now we going to be importing from Tik token the cl00 K base and uh we just writing the encoder and you can see that uh we can specify the name we can specify the pattern string then mergeable ranks and some of the
(2:13:49) stuff that's there we can add I am start I am end I am is basically when we talk to char GPD and stuff so there's a message start and message end tokens that is there that is also added so that's that's one way of using the tokens that are already there now what this means basically is that if you get a really good BP from uh gpts and others would rather use that they would have taken care of a lot of different things and much better now Google is Google which means that they have to come up with something of their own and
(2:14:18) they have something called sentence piece the sentence piece is uh similar to to to tick token but unlike also because you can efficiently both train and inference BP token it is used in L and mol is open source big difference is that it runs BP on Unicode points directly has an option of character coverage and so on so basically one more way of using it but we have not seen any uh major difference on uh sentence piece compared to Tik token people still stick to Tik token all right and the way you stick token is
(2:14:52) going to be sorry the way use sentence piece is going to be import sentence piece as SPM and we have not done a Pap install which means sentence piece comes free in collab of course it does own by Google so they going to add the services even if you don't want to read them then we're going to read that particular text and look at the options that I have for sentence piece just like Google just like AWS right what is a server what is a IP address and so on so on model prefix to lot of lot of stuff and then
(2:15:20) finally you can run it and maybe download the model or use a particular model and look at the dictionary for that also right stick to Tik token that's going to be keeping a life easy so but still just use it so if we have we saying please encode hello is vaka and print IDs and it's going to come back and not really compared to Tik token great here as well and uh if you want to print out the IDS directly it's going to print out those numbers that there are we good okay so that's all that we have in
(2:15:59) the B P en coding now what I'm going to do I'm going to run this video because there's a part of it that that I want you to understand from so in this video I'd like us to I wanted to understand from his window here hope you can see and here now can you hear to cover the process of tokenization in large language models no no [Music] one now you can hear and
(2:17:38) yeah he Mission briefly okay now how it works quite deep into the tokenization algorithm and we understand a lot more about how it works let's loop back around to the beginning of this video and and go through some of these bullet points and really see why they happen so first of all why can't my llm spell words very well or do other spell related tasks so fundamentally this is because as we saw these characters are chunked up into tokens and some of these tokens are actually fairly long so as an example I went to the gp4 vocabulary and
(2:18:13) I looked at uh one of the longer tokens so do default style turns out to be a single individual token so that's a lot of characters for a single token so my suspicion is that there's just too much crammed into this single token and my suspicion was that the model should not be very good at task related to spelling of this uh single token so I asked how many letters L are there in the word default style and of course my prompt is intentionally done that way and you see how theault style will be a single token so this is what the model
(2:18:48) sees so my suspicion is that it wouldn't be very good at this and and indeed it is not it doesn't actually know how many L's are in there it thinks there are three and actually there are four if I'm not getting this wrong myself so that didn't go extremely well let's look at look at another kind of uh character level task so for example here I asked uh gp4 to reverse the string default style and they tried to use a code interpreter and I stopped it and I said just do it just try it and uh it gave me
(2:19:18) jumble so it doesn't actually really know how to reverse this string going from right to left uh so it gave a wrong result so again like working with this working hypothesis that maybe this is due to the tokenization I tried a different approach I said okay let's reverse the exact same string but take the following approach step one just print out every single character separated by spaces and then as a step two reverse that list and it again tried to use the tool but when I stopped it it uh first uh produced all the characters
(2:19:49) and that was actually correct and then it revers them and that was correct once it had this so somehow it can't reverse it directly but when you go just first um you know listing it out in order it can do that somehow and then it can once it's uh broken up this way this becomes all these individual characters and so now this is much easier for it to see these individual tokens and reverse them in print amount so that is kind of interesting so let's continue now why are llms worse at uh non-english languages and I briefly
(2:20:21) covered this already but basically um it's not only that the language model sees less non-english data during training of the model parameters but also the tokenizer is not um is not sufficiently trained on non-english data and so here for example hello how are you is five tokens and its translation is 15 tokens so this is a three times blow up and so for example anang is uh just hello basically in Korean and that ends up being three tokens I'm actually kind of surprised by that because that is a very common phrase there just a
(2:20:55) typical greeting of like hello and that ends up being three tokens whereas our hello is a single token and so basically everything is a lot more bloated and diffuse and this is I think partly the reason that the model Works worse on other languages uh coming back why is LM bad at simple arithmetic um that has to do with the tokenization of numbers and so um you'll notice that for example addition is very sort of like like uh there's an algorithm that is like character level for doing addition so for example here we would first add
(2:21:28) the ones and then the tens and then the hundreds you have to refer to specific parts of these digits but uh these numbers are represented completely arily based on whatever happened to merge or not merge during the tokenization process there's an entire block post about this that I think is quite good integer tokenization is insane and this person basically systematically explores the tokenization of numbers in I believe this is gpt2 and so they notice that for example for the for um four-digit numbers you can take a look at whether
(2:21:58) it is uh a single token or whether it is two tokens that is a 1 three or a 22 or a 31 combination and so all the different numbers are all the different combinations and you can imagine this is all completely arbitrarily so and the model unfortunately sometimes sees uh for um a token for for all four digits sometimes for three sometimes for two sometimes for one and it's in an arbitrary uh matter and so this is definitely a headwind if you will for the language model and it's kind of incredible that it can kind of do it and it but it's
(2:22:29) also kind of not ideal and so that's why for example we saw that meta when they train the Llama 2 algorithm and use sentence piece they make sure to split up all the um all the digits as an example for uh llama 2 and this is partly to improve a simple arithmetic kind of performance and finally why is gpt2 not as good in Python again this is partly a modeling issue on in the architecture and the data set and the strength of the model but it's also partly tokenization because as we saw here with the simple
(2:23:01) python example the encoding efficiency of the tokenizer for handling spaces in Python is terrible and every single space is an individual token and this dramatically reduces the context length that the model can atttend to cross so that's almost like a tokenization bug for gpd2 and that was later fixed with gp4 okay so here's another fun one my llm abruptly halts when it sees the string end of text so here's um here's a very strange Behavior print a string end of text is what I told jup 4 and it says
(2:23:32) could you please specify the string and I'm I'm telling it give me end of text and it seems like there's an issue it's not seeing end of text and then I give it end of text is the string and then here's a string and then it just doesn't print it so obviously something is breaking here with respect to the handling of the special token and I don't actually know what open ey is doing under the hood here and whether they are potentially parsing this as an um as an actual token instead of this
(2:24:00) just being uh end of text um as like individual sort of pieces of it without the special token handling logic and so it might be that someone when they're calling do encode uh they are passing in the allowed special and they are allowing end of text as a special character in the user prompt but the user prompt of course is is a sort of um attacker controlled text so you would hope that they don't really parse or use special tokens or you know uh from that kind of input but it appears that there's something definitely going wrong
(2:24:32) here and um so your knowledge of these special tokens ends up being in a attack surface potentially and so if you'd like to confuse uh llms then just um try to give them some special tokens and see if you're breaking something by chance okay so this next one is a really fun one uh the trail whites space issue so if you come to playground and uh we come here to GPT 3.
(2:24:59) 5 turbo instruct so this is not a chat model this is a completion model so think of it more like it's a lot more closer to a base model it does completion it will continue the token sequence so here's a tagline for ice cream shop and we want to continue the sequence and so we can submit and get a bunch of tokens okay no problem but now suppose I do this but instead of pressing submit here I do here's a tag for ice cream shop space so I have a space here before I click submit we get a warning your text ends in a trail Ling space which causes worse
(2:25:34) performance due to how API splits text into tokens so what's happening here it still gave us a uh sort of completion here but let's take a look at what's happening so here's a tagline for an ice cream shop and then what does this look like in the actual training data suppose you found the completion in the training document somewhere on the internet and the llm trained on this data so maybe it's something like oh yeah maybe that's the tagline that's a terrible tagline but notice here that when I create o you
(2:26:05) see that because there's the the space character is always a prefix to these tokens in GPT so it's not an O token it's a space o token the space is part of the O and together they are token 8840 that's space o so what's What's Happening Here is that when I just have it like this and I let it complete the next token it can sample the space o token but instead if I have this and I add my space then what I'm doing here when I encode this string is I have basically here's a t line for an
(2:26:39) ice cream uh shop and this space at the very end becomes a token 220 and so we've added token 220 and this token otherwise would be part of the tagline because if there actually is a tagline here so space o is the token and so this is suddenly out of distribution for the model because this space is part of the next token but we're putting it here like this and the model has seen very very little data of actual Space by itself and we're asking it to complete the sequence like add in more tokens but the problem is that
(2:27:14) we've sort of begun the first token and now it's been split up and now we're out of distribution and now arbitrary bad things happen and it's just a very rare example for it to see something like that and uh that's why we get the warning so the fundamental issue here is of course that um the llm is on top of these tokens and these tokens are text chunks they're not characters in a way you and I would think of them they are these are the atoms of what the LM is seeing and there's a bunch of weird
(2:27:42) stuff that comes out of it let's go back to our a default cell style I bet you that the model has never in its training set seen default cell sta without Le in there it's always seen this as a single group because uh this is some kind of a function in um I'm guess I don't actually know what this is part of this is some kind of API but I bet you that it's never seen this combination of tokens uh in its training data because or I think it would be extremely rare so I took this and I copy pasted it here
(2:28:15) and I had I tried to complete from it and that it immediately gave me a big error and it said the model predicted to completion that begins with a stop sequence resulting in no output consider adjusting your prompt or stop sequences so what happened here when I clicked submit is that immediately the model emitted and sort of like end of text token I think or something like that it basically predicted the stop sequence immediately so it had no completion and so this is why I'm getting a warning again because we're off the data
(2:28:42) distribution and the model is just uh predicting just totally arbitrary things it's just really confused basically this is this is giving it brain damage it's never seen this before it's shocked and it's predicting end of text or something I tried it again here and it in this case it completed it but then for some reason this request May violate our usage policies this was flagged um basically something just like goes wrong and there's something like Jank you can just feel the Jank because
(2:29:10) the model is like extremely unhappy with just this and it doesn't know how to complete it because it's never occurred in the training set in the training set it always appears like this and becomes a single token so these kinds of issues where tokens are either you sort of like complete the first character of the next token or you are sort of you have long tokens that you then have just some of the characters off all of these are kind of like issues with partial tokens is how I would describe it and if you
(2:29:38) actually dig into the T token repository go to the rust code and search for unstable and you'll see um encode unstable native unstable tokens and a lot of like special case handling none of this stuff about unstable tokens is documented anywhere but there's a ton of code dealing with unstable tokens and unstable tokens is exactly kind of like what I'm describing here what you would like out of a completion API is something a lot more fancy like if we're putting in default cell sta if we're
(2:30:09) asking for the next token sequence we're not actually trying to append the next token exactly after this list we're actually trying to append we're trying to consider lots of tokens um that if we were or I guess like we're trying to search over characters that if we roken IED would be of high probability if that makes sense um so that we can actually add a single individual character uh instead of just like adding the next fold token that comes after this partial token list so I this is very tricky to
(2:30:40) describe and I invite you to maybe like look through this it ends up being extremely gnarly and hairy kind of topic it and it comes from tokenization fundamentally so um maybe I can even spend an entire video talking about unstable tokens sometime in the future okay and I'm really saving the best for last my favorite one by far is the solid gold magicarp and it just okay so this comes from this block post uh solid gold Magikarp and uh this is um internet famous now for those of us in llms and basically I I would advise you to uh
(2:31:12) read this block Post in full but basically what this person was doing is this person went to the um token embedding stable and clustered the tokens based on their embedding representation and this person noticed that there's a cluster of tokens that look really strange so there's a cluster here at rot e stream Fame solid gold Magikarp Signet message like really weird tokens in uh basically in this embedding cluster and so what are these tokens and where do they even come from like what is solid gold magicarp it
(2:31:45) makes no sense and then they found a bunch of these tokens and then they notice that actually the plot thickens here because if you ask the model about these tokens like you ask it uh some very benign question like please can you repeat back to me the string sold gold Magikarp uh then you get a variety of basically totally broken llm Behavior so either you get evasion so I'm sorry I can't hear you or you get a bunch of hallucinations as a response um you can even get back like insults so you ask it uh about streamer
(2:32:19) bot and it uh tells the and the model actually just calls you names uh or it kind of comes up with like weird humor like you're actually breaking the model by asking about these very simple strings like at Roth and so gold Magikarp so like what the hell is happening and there's a variety of here documented behaviors uh there's a bunch of tokens not just so good Magikarp that have that kind of a behavior and so basically there's a bunch of like trigger words and if you ask the model about these trigger words or you just
(2:32:47) include them in your prompt the model goes haywire and has all kinds of uh really Strange Behaviors including sort of ones that violate typical safety guidelines uh and the alignment of the model like it's swearing back at you so what is happening here and how can this possibly be true well this again comes down to tokenization so what's happening here is that so gold Magikarp if you actually dig into it is a Reddit user so there's a / gold Magikarp and probably what happened here uh even though I I don't know that this
(2:33:18) has been like really definitively explored but what it thought to have happened is that the tokenization data set was very different from the training data set for the actual language model so in the tokenization data set there was a ton of redded data potentially where the user so gold Magikarp was mentioned in the text because Sol gold Magikarp was a very common um sort of uh person who would post a lot uh this would be a string that occurs many times in a tokenization data set because it occurs many times in a tokenization data
(2:33:50) set these tokens would end up getting merged to the single individual token for that single Reddit user sold gold Magikarp so they would have a dedicated token in a vocabulary of was it 50,000 tokens in gpd2 that is devoted to that Reddit user and then what happens is the tokenization data set has those strings but then later when you train the model the language model itself um this data from Reddit was not present and so therefore in the entire training set for the language model solid gold Magikarp never occurs that
(2:34:23) token never appears in the training set for the actual language model later so this token never gets activated it's initialized at random in the beginning of optimization then you have forward backward passes and updates to the model and this token is just never updated in the embedding table that row Vector never gets sampled it never gets used so it never gets trained and it's completely untrained it's kind of like unallocated memory in a typical binary program written in C or something like that so it's unallocated memory and then
(2:34:51) at test time if you evoke this token then you're basically plucking out a row of the embedding table that is completely untrained and that feeds into a Transformer and creates undefined behavior and that's what we're seeing here this completely undefined never before seen in a training behavior and so any of these kind of like weird tokens would evoke this Behavior because fundamentally the modelis um is uh uh out of sample out of distribution okay and the very last thing I wanted to just briefly mention and point out although I
(2:35:19) think a lot of people are quite aware of this is yeah so we're back now but I think you get the idea so do look at this uh whole video it's amazing and it will allow you to get more deeper better understanding and he also funny so that's also good so look at this video full video in fact what we have covered is uh completely inspired from him I'm not going to uh shy away from saying he's one of the best AI tutor on planet so any video you find of him go through that the next video that we're going to cover is also
(2:35:54) inspired from what he's going to do in his space but I'm also going to be adding some of the other stuff that you saw from the extreme compression that we are uh in the training time that we are seeing so your assignment now is to pick any Indian language it has to be Indian language English is not Indian language even though we are the largest speakers of this language on the planet uh pick an Indian language build your bpe must have less than 5,000 tokens uh must have a compression ratio of 3.2 or above you
(2:36:21) need to share your GitHub link then you convert that into the app and upload to hugging face spaces and share with me on your read me I need to see the token count is what is the token count and what is the compression ratio and also looking at your rejects uh and if you're not taken care of something specific from Indian context then essentially the assignment gets a score of zero so you need to come up with a anch can help you come with a special rejects for Indian languages to make sure that you actually
(2:36:47) treating it like your own baby not some sort of import from China okay so that's where the session ends today uh it's very simple stuff what we've discuss but indirectly what this session does is it forces you to think about what is happening on the llm space what is the data that's going to be sent in What are problems we going to look at so indirectly we've actually covered a lot of uh uh model architecture already now you know that your text whatever you're going to be sending in is going to be
(2:37:15) converted into sequence of tokens right and uh the problem that we are trying to solve here is that we do not want to represent the words directly into tokens because then we have 1.5 million and 7.5 million sorry and then uh Python Programming and all the other stuff that we have to take care of so we have to come up with something very specific we can't use characters because then there are just going to be too many characters to deal with so we need to create a space that is going to be somewhere in between and currently that space is
(2:37:41) created by bpe that is send data through some sort of reject right so language sent to reject send to BP that gives us 50k or 100K tokens that we use so all the text that we are sending to our llm is converted into this BP text and this is the input to LM now each token will have its own 768 or one24 or 2048 depending on the model we're talking about eding right so even though uh we saw isation has a specific token let's say 12693 but this is still going to be converted into 768 floating numbers right that's the embedding we have so
(2:38:23) the input to model is large but it's a sequence of text that we sending in and this text is already converted into tokens now that is going to be sent to our llm and we'll have llm blocks repeating let's say 12 times or eight times or six times depending on which model we're talking about and then element is going to predict one word and that word is also going to be one of the words from this to dictionary of 50k or 100K that we're talking about right so one of the words from this gets predicted next
(2:38:50) so overall architecture If You Now understand is text uh tokenized so many ways of tokenization to TI token 50k uh then we have something from Google also so tokenized and then send to our llm architecture that's what we're going to be discussing in the next session and LM predicts one word which is again going to be one of the word from the tokens that we have two original dictionary basically and inside we have this KQ V model masking and a lot of other stuff that's what we're going to be discussing
(2:39:24) in the next session so today's assignment is for you to be here look at any indic language and write a tokenizer for that maximum 5,000 tokens so 5,000 would mean that 5,000 minus 256 on 257 so 4,743 times you can run that algorithm but your reject also need to make sure that you have taken care of some of the interesting things that you you may find in the Indian language all right so that's where the session ends if you have any questions then I'm here is there any intelligence in token merging into word on the LM output as
(2:40:01) not no SATA uh how about the data set like from where will run this you will have to find online there lot of data sets on hugging face or you can just download 40 pages of Wikipedia and Canada okay hey everyone I have a question about previous assignment if anyone has questions like the session n let's wait for five minutes Nishant uh one is there do we need to do something special to convert the data tokenizer into hugging face format not really you can just ask charg itself to give you that code it's really simple we
(2:40:54) have seen that actually so some text comes in and goes through this thing and we get the tokens out this is this is all basically okay is it hi yeah hi R uh so that 5,000 tokens limit right so that means like whatever the data set that we are going to choose for the assignment should have around uh uh 5,000 words no we saw today we can have hundreds and thousands of words we just run the algorithm 5,000 times okay so is there a minimum number of words limit for choosing the data set now no we saw right we have 1 7.5
(2:41:36) million words but I tokenize it's just 50,000 okay okay uh for the res part in any indic language so how will you do that like you cannot find the characters right or you use the like whatever characters copy that and you understand what I'm asking right so like in that is the assignment okay okay so work with ch are clot free and try and figure out if there something can be done okay here uh hi Rohan Rohan is the page visible now for me it's still logged this page I think we will be visible at 11: session 11 yes yeah it
(2:42:32) will be visible by 11: thank you I didn't see that part sorry an yeah hi Ron so just for an analogy for in OCR uh for example the tesser act and other libraries that are they are trained on some other Vision models right so that they can extract the uh characters just like that for embedding and this uh you know tokenization uh for indic languages we need to some import some other pre-train model or I will not solve the assignment for you but you need to talk to llms and figure out something very specific now
(2:43:20) you can actually get some words worked out by Rex library but the tokenizer as you said it does not work on any pre-trained AI model right it is purely algorithmic like corre so you have to apply your own mind and come up with a pattern that works with inding languages okay okay but not necessarily importing any other you know pre-train models no no you're you're building one from scratch okay so for rejects as I you know as I read it is used for lization right like text cleaning and all for example if we have know unwanted
(2:44:07) raw data in the text the rejects will remove that know comma apostrophe space that's not what we did here we captured all we split the word based on our understanding of English right we did not throw a posts or exclamation or the stuff we we captured them we split the word we are using rejects here for splitting up not for litiz or other stuff you can you use a reject for that yes you can but we are not doing it for that we are using it to split our words into something that is manageable okay so here also you can see look can
(2:44:49) have commas you can have ending this so you need to come up with your own idea of CH hion uh so just to understand it correctly uh whatever we have seen today just about the tokens now each token will have whatever Dimensions we decide so that will be embedding and uh creating the embedding is a separate part from from the tokenization part is that correct correct thank you all right you can go back to Session 9 if you have any questions for that uh okay actually it was Session 8 the cifar 10 and said I had a question
(2:45:44) so if other have done I can okay yeah so in that you had limits of 200k parameters and to reach 80% accuracy as many um 80% test accuracy as many uh EPO as possible so I I I I tried different ways but my training accuracy got stuck at like 75 or 70 so I try to follow some of the strategies that you had said like those depth way separate convolution and all that so how should I approach this like if I have to solve a real world problem like my training accuracy got stuck at 80 I will never get to test accuracy of 80% so how do I go about it
(2:46:27) I will share some of the solutions for that and uh the solution lies in your model architecture is it similar to resonant or not and second is the augmentation and third most important thing that I think you have not done is have you looked at images you're not able to predict properly uh okay yeah I have not okay so I have to do that visualize the failures yes there you understand okay you need rotation or you need drop out uh sorry cut out or do you need flipping of those images that will give you a really good
(2:46:56) idea okay okay thanks okay than you uh so hi so I was I was uh for assignment 9 I was doing uh multi-gpu training uh for single instance uh multi GPU training but uh I I was not able to do it uh like it was not learning l so is there any technique that we need to do for gradient like the back propagation to work properly on the multi GPU uh J if you write the code properly then it should work by default but we have a session on multipu training also okay and the simp approach is to convert your code into py lightning and
(2:47:39) it will take care of everything for you yeah that's that was my second point I was doing the second approach I tried was the lightning still it was not doing properly configuration level okay I use the strategy to DDP uh and uh I also added one parameter which is for the how to do the batch Norm in the multi GPU setting that parameters also enabled it but still it was not moving forward then I switched to the single GPU and it finished in 4 and one thing that I noticed is uh so first of all let's say I uploaded a
(2:48:24) turtle image and it predicted Turtle correctly but then I took the screenshot of the entire screen and I uploaded then it predicted the website as a first class and the second class was Turtle so that's correct right yeah but uh what if we want to make it Turtle like is there any way we we need to train you have to move the website class website yeah okay okay okay than right let's say we have a man and apple and man is wearing a photo of an apple because man is bigger you're going to get man as a
(2:48:59) first prediction apple is going to be second prediction because the amplitude Okay so that behavior is correct right I mean
